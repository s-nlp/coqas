{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: ../github/allennlp/ is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with svn+, git+, hg+, or bzr+).\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ../github/allennlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: allennlp\n",
      "Version: 1.3.0\n",
      "Summary: An open-source NLP research library, built on PyTorch.\n",
      "Home-page: https://github.com/allenai/allennlp\n",
      "Author: Allen Institute for Artificial Intelligence\n",
      "Author-email: allennlp@allenai.org\n",
      "License: Apache\n",
      "Location: /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages\n",
      "Requires: scikit-learn, numpy, filelock, boto3, scipy, overrides, spacy, requests, jsonpickle, tensorboardX, pytest, transformers, torch, nltk, tqdm, jsonnet, sentencepiece, h5py\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip3 show allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2d9fa626d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 28 09:50:56 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 68%   67C    P2   241W / 260W |  10805MiB / 11019MiB |     95%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 70%   68C    P2   268W / 260W |  10586MiB / 11019MiB |     95%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   36C    P8     6W / 260W |      9MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8    10W / 260W |      9MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 27%   25C    P8     2W / 260W |      9MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  Off  | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 27%   29C    P8    14W / 260W |      9MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  Off  | 00000000:40:00.0 Off |                  N/A |\n",
      "| 27%   27C    P8    10W / 260W |   3154MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  Off  | 00000000:41:00.0 Off |                  N/A |\n",
      "| 27%   27C    P8    15W / 260W |      9MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "cuda_device = torch.device('cuda:2')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "for i in range(n_gpu):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allennlp.models.simple_tagger.SimpleTagger"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting allennlp\n",
      "  Using cached https://files.pythonhosted.org/packages/72/f5/f4dd3424b3ae9dec0a55ae7f7f34ada3ee60e4b10a187d2ba7384c698e09/allennlp-1.3.0-py3-none-any.whl\n",
      "Collecting nltk\n",
      "  Using cached https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip\n",
      "Collecting numpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/bb/87d668b353848b93baab0a64cddf6408c40717f099539668c3d26fe39f7e/numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5MB 172kB/s eta 0:00:01    |█████████▌                      | 4.3MB 1.4MB/s eta 0:00:08\n",
      "\u001b[?25hCollecting h5py\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/74/9eae2bedd8201ab464308f42c601a12d79727a1c87f0c867fdefb212c6cf/h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0MB 21.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.19\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/bb/9403e1f30ed060e16835c9b275620ca89191a41ccc2b995b88efbc32dfd9/tqdm-4.55.0-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 19.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers<4.1,>=4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 25.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy<2.4,>=2.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/89/1539c4024c339650c222b0b2ca2b3e3f13523b7a02671f8001b7b1cee6f2/spacy-2.3.5-cp37-cp37m-manylinux2014_x86_64.whl (10.4MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4MB 17.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests>=2.18\n",
      "  Using cached https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/ed/ab51a8da34d2b3f4524b21093081e7f9e2ddf1c9eac9f795dcf68ad0a57d/scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3MB 53.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/7e/8f6a79b102ca1ea928bae8998b05bf5dc24a90571db13cd119f275ba6252/scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9MB 39.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 39.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch<1.8.0,>=1.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
      "\u001b[K     |████████████████████████████████| 776.8MB 39kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboardX>=1.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 31.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock<3.1,>=3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Collecting pytest\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/15/5ef931cbd22585865aad0ea025162545b53af9319cf38542e0b7981d5b34/pytest-6.2.1-py3-none-any.whl (279kB)\n",
      "\u001b[K     |████████████████████████████████| 286kB 41.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting overrides==3.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
      "Collecting jsonpickle\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/d5/1cc282dc23346a43aab461bf2e8c36593aacd34242bee1a13fa750db0cfe/jsonpickle-1.4.2-py2.py3-none-any.whl\n",
      "Collecting boto3<2.0,>=1.14\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/c6/912cc2cfd1b4051621552fc5961c25e2f517a090d179a38f62d5cdaf5d37/boto3-1.16.43-py2.py3-none-any.whl (130kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 25.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Using cached https://files.pythonhosted.org/packages/6e/f0/7614029138ec9422f1a3ed3cd82c3bfc0821157e8032ca1828cee6b198bb/sentencepiece-0.1.94-cp37-cp37m-manylinux2014_x86_64.whl\n",
      "Collecting click\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 21.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/5b/bd0f0fb5564183884d8e35b81d06d7ec06a20d1a0c8b4c407f1554691dce/joblib-1.0.0-py3-none-any.whl (302kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 36.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/b2/8f281520d9f08d0f6771b8160a87a4b487850cde9f1abe257da4d8bab599/regex-2020.11.13-cp37-cp37m-manylinux2014_x86_64.whl (719kB)\n",
      "\u001b[K     |████████████████████████████████| 727kB 42.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cached-property; python_version < \"3.8\"\n",
      "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 29.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.9.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/36/59e4a62254c5fcb43894c6b0e9403ec6f4238cc2422a003ed2e6279a1784/tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 33.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/a7/588bfa063e7763247ab6f7e1d994e331b85e0e7d09f853c59a6eb9696974/packaging-20.8-py2.py3-none-any.whl\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/82/8a995c3a2c01f4fe750ed25de26755b103b9669a9113272c395c560d021f/murmurhash-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/48/44bd8693a7a705976884cabd52a516fe5cbcecf5f45be732f8f04ad2605b/srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 26.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<7.5.0,>=7.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/92/71ab278f865f7565c37ed6917d0f23342e4f9a0633013113bd435cf0a691/thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 39.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/3c/096ac70aeb74485a2e7ab6f8146ea41cb4cecd955fdb2f30d51b51d7d146/setuptools-51.1.0-py3-none-any.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 36.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/10/55f3cf6b52cc89107b3e1b88fcf39719392b377a3d78ca61da85934d0d10/wasabi-0.8.0-py3-none-any.whl\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/c1/f364687078298233696eff17305f9a54c4a27d9da03c07c0062909d550f1/blis-0.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.8MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8MB 22.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/b6/34/40547e057c1b31080c1d78f6accf9f1ed6ee46e3fc7ebd8599197915ef89/cymem-2.0.5-cp37-cp37m-manylinux2014_x86_64.whl\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/91/1cf0f7f0a6720f93632fc8ec42d54233e8e142640ac3fcf0fecaa8dc4648/preshed-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (126kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 47.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<5,>=3.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/c7/fa589626997dd07bd87d9269342ccb74b1720384a4d739a1872bd84fbe68/chardet-4.0.0-py2.py3-none-any.whl (178kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 33.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl (136kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 37.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<3,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 13.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/a0/5f06e1e1d463903cf0c0eebeb751791119ed7a4b3737fdc9a77f1cdfb51f/certifi-2020.12.5-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 54.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Collecting typing-extensions\n",
      "  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Collecting six\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.8.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/dd/5c5d156ee1c4dba470d76dac5ae57084829b4e17547f28e9f636ce3fa54b/protobuf-3.14.0-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 36.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting iniconfig\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/dd/b3c12c6d707058fa947864b67f0c4e0c39ef8610988d7baea9578f3c48f3/iniconfig-1.1.1-py2.py3-none-any.whl\n",
      "Collecting toml\n",
      "  Downloading https://files.pythonhosted.org/packages/44/6f/7120676b6d73228c96e17f1f794d8ab046fc910d781c8d151120c3f1569e/toml-0.10.2-py2.py3-none-any.whl\n",
      "Collecting attrs>=19.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/aa/cb45262569fcc047bf070b5de61813724d6726db83259222cd7b4c79821a/attrs-20.3.0-py2.py3-none-any.whl (49kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 13.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py>=1.8.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/32/6fe01cfc3d1a27c92fdbcdfc3f67856da8cbadf0dd9f2e18055202b2dc62/py-1.10.0-py2.py3-none-any.whl (97kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 17.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pluggy<1.0.0a1,>=0.12\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
      "Collecting importlib-metadata>=0.12; python_version < \"3.8\"\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/85/ac225e35048e050a6351b6f1251cdb2b6060092f2c6840aff1d6319941b1/importlib_metadata-3.3.0-py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 20.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.20.0,>=1.19.43\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/6c/9f6e6a14f53b21b6f1670ccd789082015911458823914b7dabca333ae033/botocore-1.19.43-py2.py3-none-any.whl (7.2MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2MB 25.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 15.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zipp>=0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/41/ad/6a4f1a124b325618a7fb758b885b68ff7b058eec47d9220a12ab38d90b1f/zipp-3.4.0-py3-none-any.whl\n",
      "Collecting python-dateutil<3.0.0,>=2.1\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: nltk, jsonnet, overrides, sacremoses\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-cp37-none-any.whl size=1434676 sha256=29995232495e05f58f58b9aacd06119b1c22afc993b541ad025b9ee113b0dbf3\n",
      "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3392602 sha256=3a4d491d888bcbc861312617b09215601d273f22e3f16891eb820e38bfeefc0a\n",
      "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=76ada608d1f5cb0341f7c80818418aa088e43f21625ec083bb7adbe020c789e4\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893261 sha256=96dcd09f2170a8d1543db506b0a74447169a4ca4b06ccf43c34d489ded158b7b\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built nltk jsonnet overrides sacremoses\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 requires fastapi==0.38.1, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 requires flasgger==0.9.2, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 requires fuzzywuzzy==0.17.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 requires pymorphy2==0.8, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 requires pymorphy2-dicts-ru, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 requires pyopenssl==19.0.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 requires pytelegrambotapi==3.6.6, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 requires rusenttokenize==0.0.5, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 requires uvicorn==0.9.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: torchvision 0.4.2 has requirement torch==1.3.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: sacred 0.8.0 has requirement jsonpickle<1.0,>=0.7.2, but you'll have jsonpickle 1.4.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 has requirement Cython==0.29.12, but you'll have cython 0.29.14 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 has requirement h5py==2.9.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 has requirement keras==2.2.4, but you'll have keras 2.3.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 has requirement nltk==3.2.5, but you'll have nltk 3.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 has requirement numpy==1.16.4, but you'll have numpy 1.19.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 has requirement overrides==1.9, but you'll have overrides 3.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 has requirement pandas==0.24.2, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 has requirement requests==2.22.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 has requirement scikit-learn==0.21.2, but you'll have scikit-learn 0.24.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 has requirement scipy==1.3.0, but you'll have scipy 1.5.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.6.1 has requirement tqdm==4.32.2, but you'll have tqdm 4.55.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: click, joblib, regex, tqdm, nltk, numpy, cached-property, h5py, chardet, urllib3, idna, certifi, requests, filelock, six, sacremoses, tokenizers, pyparsing, packaging, transformers, murmurhash, srsly, cymem, preshed, blis, wasabi, typing-extensions, zipp, importlib-metadata, catalogue, plac, thinc, setuptools, spacy, scipy, threadpoolctl, scikit-learn, jsonnet, torch, protobuf, tensorboardX, iniconfig, toml, attrs, py, pluggy, pytest, overrides, jsonpickle, jmespath, python-dateutil, botocore, s3transfer, boto3, sentencepiece, allennlp\n",
      "Successfully installed allennlp-1.3.0 attrs-20.3.0 blis-0.7.4 boto3-1.16.43 botocore-1.19.43 cached-property-1.5.2 catalogue-1.0.0 certifi-2020.12.5 chardet-4.0.0 click-7.1.2 cymem-2.0.5 filelock-3.0.12 h5py-3.1.0 idna-2.10 importlib-metadata-3.3.0 iniconfig-1.1.1 jmespath-0.10.0 joblib-1.0.0 jsonnet-0.17.0 jsonpickle-1.4.2 murmurhash-1.0.5 nltk-3.5 numpy-1.19.4 overrides-3.1.0 packaging-20.8 plac-1.1.3 pluggy-0.13.1 preshed-3.0.5 protobuf-3.14.0 py-1.10.0 pyparsing-2.4.7 pytest-6.2.1 python-dateutil-2.8.1 regex-2020.11.13 requests-2.25.1 s3transfer-0.3.3 sacremoses-0.0.43 scikit-learn-0.24.0 scipy-1.5.4 sentencepiece-0.1.94 setuptools-51.1.0 six-1.15.0 spacy-2.3.5 srsly-1.0.5 tensorboardX-2.1 thinc-7.4.5 threadpoolctl-2.1.0 tokenizers-0.9.4 toml-0.10.2 torch-1.7.1 tqdm-4.55.0 transformers-4.0.1 typing-extensions-3.7.4.3 urllib3-1.26.2 wasabi-0.8.0 zipp-3.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install allennlp --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08509ae0191643e0a7ddc794e4a61ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/467 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559a7d1d75cc4174bd301c04985fbde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cf74fec1574a6c859bfc040d129550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from allennlp.data.token_indexers import PretrainedTransformerMismatchedIndexer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "#BERT_MODEL = 'bert-base-cased'\n",
    "BERT_MODEL = 'google/electra-base-discriminator'\n",
    "indexer = PretrainedTransformerMismatchedIndexer(model_name=BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Sequence, Iterable\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.dataset_readers.dataset_utils import to_bioul\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, Field, MetadataField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _is_divider(line: str) -> bool:\n",
    "    empty_line = line.strip() == \"\"\n",
    "    if empty_line:\n",
    "        return True\n",
    "    else:\n",
    "        first_token = line.split()[0]\n",
    "        if first_token == \"-DOCSTART-\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "class ConllUniversalReader(DatasetReader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_indexers: Dict[str, TokenIndexer] = None,\n",
    "        tag_index: int = 0,\n",
    "        coding_scheme: str = \"IOB1\",\n",
    "        label_namespace: str = \"labels\",\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        \n",
    "        if coding_scheme not in (\"IOB1\", \"BIOUL\"):\n",
    "            raise ConfigurationError(\"unknown coding_scheme: {}\".format(coding_scheme))\n",
    "\n",
    "        self.tag_index = tag_index\n",
    "        self.coding_scheme = coding_scheme\n",
    "        self.label_namespace = label_namespace\n",
    "        self._original_coding_scheme = \"IOB1\"\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        # if `file_path` is a URL, redirect to the cache\n",
    "        file_path = cached_path(file_path)\n",
    "\n",
    "        with open(file_path, \"r\") as data_file:\n",
    "            logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "\n",
    "            # Group into alternative divider / sentence chunks.\n",
    "            for is_divider, lines in itertools.groupby(data_file, _is_divider):\n",
    "                # Ignore the divider chunks, so that `lines` corresponds to the words\n",
    "                # of a single sentence.\n",
    "                if not is_divider:\n",
    "                    fields = [line.strip().split() for line in lines]\n",
    "                    # unzipping trick returns tuples, but our Fields need lists\n",
    "                    fields = [list(field) for field in zip(*fields)]\n",
    "                    tokens_ = fields[0]\n",
    "                    if self.tag_index >= 0:\n",
    "                        ner_tags = fields[1:][self.tag_index]\n",
    "                    else:\n",
    "                        ner_tags = None\n",
    "                    # TextField requires `Token` objects\n",
    "                    tokens = [Token(token) for token in tokens_]\n",
    "\n",
    "                    yield self.text_to_instance(tokens, ner_tags)\n",
    "\n",
    "    def text_to_instance(  # type: ignore\n",
    "        self,\n",
    "        tokens: List[Token],\n",
    "        ner_tags: List[str] = None,\n",
    "    ) -> Instance:\n",
    "        \"\"\"\n",
    "        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n",
    "        \"\"\"\n",
    "\n",
    "        sequence = TextField(tokens, self._token_indexers)\n",
    "        instance_fields: Dict[str, Field] = {\"tokens\": sequence}\n",
    "        instance_fields[\"metadata\"] = MetadataField({\"words\": [x.text for x in tokens]})\n",
    "\n",
    "        # Recode the labels if necessary.\n",
    "        if self.coding_scheme == \"BIOUL\":\n",
    "            coded_ner = (\n",
    "                to_bioul(ner_tags, encoding=self._original_coding_scheme)\n",
    "                if ner_tags is not None\n",
    "                else None\n",
    "            )\n",
    "        else:\n",
    "            # the default IOB1\n",
    "            coded_ner = ner_tags\n",
    "\n",
    "        \n",
    "        # Add \"tag label\" to instance\n",
    "        if coded_ner:\n",
    "            instance_fields[\"tags\"] = SequenceLabelField(coded_ner, sequence, self.label_namespace)\n",
    "        \n",
    "        return Instance(instance_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38998506b72c462abeaa39bb4d991971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12fcd681668474c8f44fb7a97b622f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from allennlp.data.dataset_readers import Conll2003DatasetReader\n",
    "from allennlp.data.dataset_readers import SequenceTaggingDatasetReader\n",
    "\n",
    "#reader = Conll2003DatasetReader(token_indexers={'tokens': indexer})\n",
    "reader = ConllUniversalReader(token_indexers={'tokens': indexer})\n",
    "train_dataset = reader.read('train.tsv')\n",
    "dev_dataset = reader.read('dev.tsv')\n",
    "#test_dataset = reader.read('data_1/test_no_answers.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4fd5fd409946a9a5ba79ba1df9d243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/2334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset.instances)\n",
    "train_dataset.index_with(vocab)\n",
    "dev_dataset.index_with(vocab)\n",
    "#test_dataset.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a479051867264c94bcff82a6c3862ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from allennlp.modules.token_embedders import PretrainedTransformerMismatchedEmbedder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.seq2seq_encoders import PassThroughEncoder\n",
    "\n",
    "\n",
    "embedder = PretrainedTransformerMismatchedEmbedder(model_name=BERT_MODEL)\n",
    "text_field_embedder = BasicTextFieldEmbedder({'tokens': embedder})\n",
    "seq2seq_encoder = PassThroughEncoder(input_dim=embedder.get_output_dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import SimpleTagger\n",
    "\n",
    "\n",
    "model = SimpleTagger(text_field_embedder=text_field_embedder, \n",
    "                      vocab=vocab, \n",
    "                      encoder=seq2seq_encoder,\n",
    "                      calculate_span_f1=True,\n",
    "                      label_encoding='IOB1').cuda(device=cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import AdamW\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "from allennlp.training.learning_rate_schedulers import LinearWithWarmup\n",
    "from torch.utils.data import DataLoader\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "from allennlp.training.learning_rate_schedulers import SlantedTriangular\n",
    "from allennlp.data import allennlp_collate\n",
    "\n",
    "import math\n",
    "\n",
    "def train_roberta_warmup(lr, batch_size):\n",
    "    num_epochs = 3\n",
    "    #batch_size = 16\n",
    "    #batch_size = 2\n",
    "    #accum = 4\n",
    "    accum = 1\n",
    "    steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, \n",
    "                                   collate_fn=allennlp_collate, shuffle=True)\n",
    "    val_data_loader = DataLoader(dataset=dev_dataset, batch_size=batch_size, collate_fn=allennlp_collate)\n",
    "    lr_scheduler = LinearWithWarmup(optimizer, \n",
    "                                    num_epochs=num_epochs, \n",
    "                                    warmup_steps=(steps_per_epoch*num_epochs)*0.1, \n",
    "                                    num_steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "    date_time = datetime.now()\n",
    "    date_str = date_time.strftime('%m/%d/%Y')\n",
    "    time_str = date_time.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "    trainer = GradientDescentTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        patience = 1,\n",
    "        data_loader=train_data_loader,\n",
    "        validation_data_loader=val_data_loader,\n",
    "        #validation_data_loader=None,\n",
    "        num_epochs=num_epochs,\n",
    "        cuda_device=cuda_device,\n",
    "        learning_rate_scheduler=lr_scheduler,\n",
    "        num_gradient_accumulation_steps=accum,\n",
    "        serialization_dir=f'./workdir/{date_str}/{time_str}',\n",
    "        grad_clipping=1.\n",
    "    )\n",
    "    parameter_tuple = (lr, batch_size, (steps_per_epoch*num_epochs)*0.1)\n",
    "    try:\n",
    "        metrics = trainer.train()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    return metrics, parameter_tuple\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c3849f1ed7462f82bdbf46dd3cf48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9625d77ea43e4b91ac3cfde6df3c6139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960b300748354378a684804ce69f6656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa4603d9a114bf881c0cfc6764e8321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd4d64a38ed470987071f2e576427c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ff1e77758f4a2c8b0052d331b607e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import AdamW\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "from allennlp.training.learning_rate_schedulers import LinearWithWarmup\n",
    "from torch.utils.data import DataLoader\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "from allennlp.training.learning_rate_schedulers import SlantedTriangular\n",
    "from allennlp.data import allennlp_collate\n",
    "\n",
    "import math\n",
    "#torch.manual_seed(111)\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 16\n",
    "#batch_size = 2\n",
    "#accum = 4\n",
    "accum = 1\n",
    "steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=4e-5, weight_decay=0.01)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, \n",
    "                               collate_fn=allennlp_collate, shuffle=True)\n",
    "val_data_loader = DataLoader(dataset=dev_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "lr_scheduler = LinearWithWarmup(optimizer, \n",
    "                                num_epochs=num_epochs, \n",
    "                                warmup_steps=(steps_per_epoch*num_epochs)*0.1, \n",
    "                                num_steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "date_time = datetime.now()\n",
    "date_str = date_time.strftime('%m/%d/%Y')\n",
    "time_str = date_time.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=val_data_loader,\n",
    "    #validation_data_loader=None,\n",
    "    num_epochs=num_epochs,\n",
    "    cuda_device=cuda_device,\n",
    "    learning_rate_scheduler=lr_scheduler,\n",
    "    num_gradient_accumulation_steps=accum,\n",
    "    serialization_dir=f'./workdir/{date_str}/{time_str}',\n",
    "    grad_clipping=1.\n",
    ")\n",
    "\n",
    "try:\n",
    "    metrics = trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"roberta.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_minima = 2\n",
    "\n",
    "#state_dicts_at_minima = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_dicts_at_minima' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5e6fa49b9130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstate_dicts_at_minima\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'state_dicts_at_minima' is not defined"
     ]
    }
   ],
   "source": [
    "from copy import copy, deepcopy\n",
    "\n",
    "state_dicts_at_minima.append(deepcopy(model.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def interpolate_between_state_dicts(t, state_dict_a, state_dict_b):\n",
    "    state_dict = OrderedDict()\n",
    "    assert list(state_dict_a.keys()) == list(state_dict_b.keys()), 'different keys'\n",
    "    for key in list(state_dict_a.keys()):\n",
    "        state_dict[key] = t*state_dict_a[key] + (1-t)*state_dict_b[key]\n",
    "\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dict = interpolate_between_state_dicts(0.5, state_dicts_at_minima[0], state_dicts_at_minima[1])\n",
    "model.load_state_dict(current_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60f2383e0b6482cac5b1f68df695a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9182111682410619,\n",
       " 'accuracy3': 0.9968910677986369,\n",
       " 'precision-overall': 0.6968295904887715,\n",
       " 'recall-overall': 0.7408707865168539,\n",
       " 'f1-measure-overall': 0.7181756296800046,\n",
       " 'loss': 0.2511654595534007}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.training.util import evaluate\n",
    "\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "evaluate(model, dev_dataloader, cuda_device=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save_to_files(\"./vocab_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1= Vocabulary.from_files(\"./vocab_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allennlp.data.vocabulary.Vocabulary"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "class CustomSentenceTaggerPredictor(SentenceTaggerPredictor):\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict):\n",
    "        tokens = [Token(e) for e in json_dict[\"sentence\"]]\n",
    "        return self._dataset_reader.text_to_instance(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spacy models 'en_core_web_sm' not found.  Downloading and installing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Python', 'B-Object'),\n",
       " ('is', 'O'),\n",
       " ('better', 'B-Predicate'),\n",
       " ('than', 'O'),\n",
       " ('C++', 'B-Object')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = CustomSentenceTaggerPredictor(model, reader)\n",
    "preds = predictor.predict(['Python', 'is', 'better', 'than', 'C++'])\n",
    "list(zip(preds['words'], preds['tags'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'B-Object'),\n",
       " ('is', 'O'),\n",
       " ('better', 'B-Predicate'),\n",
       " ('than', 'O'),\n",
       " ('C++', 'B-Object')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "predictor = SentenceTaggerPredictor(model, reader)\n",
    "preds = predictor.predict('Python is better than C++')\n",
    "list(zip(preds['words'], preds['tags']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"/notebook/NLU_last_version/models/model_warm_up.th\", 'wb') as f:\n",
    "    #torch.save(model.state_dict(), f)\n",
    "\n",
    "#vocab.save_to_files(\"/notebook/NLU_last_version/models/vocabulary_warm_up\")\n",
    "\n",
    "#vocab2 = Vocabulary.from_files(\"/tmp/vocabulary\")\n",
    "\n",
    "#model2 = LstmTagger(word_embeddings, lstm, vocab2)\n",
    "#with open(\"/tmp/model.th\", 'rb') as f:\n",
    "    #model2.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval on the dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_labels = [list(e['tags']) for e in dev_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n"
     ]
    }
   ],
   "source": [
    "predictor = CustomSentenceTaggerPredictor(model, reader)\n",
    "\n",
    "dev_eval_loader = DataLoader(dataset=dev_dataset, batch_size=100, \n",
    "                             shuffle=False, collate_fn=lambda a: a)\n",
    "\n",
    "all_preds = []\n",
    "for batch in dev_eval_loader:\n",
    "    all_preds += predictor.predict_batch_instance(batch)\n",
    "    \n",
    "pred_tags = [pred['tags'][:len(pred['words'])] for pred in all_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7181756296800544\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "   Object       0.66      0.71      0.69       781\n",
      "   Aspect       0.55      0.57      0.56       257\n",
      "Predicate       0.87      0.91      0.89       386\n",
      "\n",
      "micro avg       0.70      0.74      0.72      1424\n",
      "macro avg       0.70      0.74      0.72      1424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "print(f1_score(dev_labels, pred_tags))\n",
    "print(classification_report(dev_labels, pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('georgia', 'B-Object'),\n",
       " ('has', 'O'),\n",
       " ('a', 'O'),\n",
       " ('higher', 'B-Predicate'),\n",
       " ('percentage', 'B-Aspect'),\n",
       " ('of', 'I-Aspect'),\n",
       " ('blacks', 'B-Aspect'),\n",
       " ('(', 'O'),\n",
       " ('30', 'O'),\n",
       " ('.', 'O'),\n",
       " ('5', 'O'),\n",
       " (')', 'O'),\n",
       " ('than', 'O'),\n",
       " ('new', 'B-Object'),\n",
       " ('york', 'B-Object'),\n",
       " ('(', 'O'),\n",
       " ('15', 'O'),\n",
       " ('.', 'O'),\n",
       " ('9', 'O'),\n",
       " (')', 'O'),\n",
       " ('or', 'O'),\n",
       " ('california', 'B-Object'),\n",
       " ('(', 'O'),\n",
       " ('6', 'O'),\n",
       " ('.', 'O'),\n",
       " ('2', 'O'),\n",
       " (').', 'O')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_num = 47\n",
    "list(zip(all_preds[sent_num]['words'], all_preds[sent_num]['tags']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0bf0bacf4d4a37a005f2f6583ee2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='reading instances'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reader = ConllUniversalReader(token_indexers={'tokens': indexer}, tag_index=-1)\n",
    "test_dataset_no_answers = reader.read('test_no_answers.tsv')\n",
    "predict_data_loader = DataLoader(dataset=test_dataset_no_answers, batch_size=100, \n",
    "                                 shuffle=False, collate_fn=lambda a: a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = CustomSentenceTaggerPredictor(model, reader) \n",
    "\n",
    "predict_data_loader = DataLoader(dataset=test_dataset_no_answers, batch_size=100, \n",
    "                                 shuffle=False, collate_fn=lambda a: a)\n",
    "\n",
    "all_preds = []\n",
    "for batch in predict_data_loader:\n",
    "    all_preds += predictor.predict_batch_instance(batch)\n",
    "    \n",
    "pred_tags = [pred['tags'][:len(pred['words'])] for pred in all_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plus', ',', 'android', 'is', 'developing', 'a', 'way', 'faster', 'than', 'ios', 'so', 'it', 'has', 'chances', 'to', 'become', 'a', 'laptop', 'replacement', 'earlier', 'than', 'ios', '.']\n",
      "['went', 'to', 'android', 'earlier', 'this', 'year', 'after', 'being', 'convinced', 'its', 'better', 'then', 'ios', 'apple', '.']\n",
      "['the', 'version', 'we', 'showed', 'here', 'is', 'ios', 'only', ',', 'because', 'the', 'ios', 'code', 'supported', 'ibeacons', 'earlier', 'than', 'android', ',', 'but', 'we', 'are', 'almost', 'finished', 'with', 'an', 'android', 'one', 'as', 'well', '.']\n"
     ]
    }
   ],
   "source": [
    "for pred in all_preds[:3]:\n",
    "    print (pred['words'])\n",
    "    a = [f'{w}\\t{t}' for w, t in zip(pred['words'], pred['tags'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the\\tO',\n",
       " 'version\\tO',\n",
       " 'we\\tO',\n",
       " 'showed\\tO',\n",
       " 'here\\tO',\n",
       " 'is\\tO',\n",
       " 'ios\\tB-Object',\n",
       " 'only\\tO',\n",
       " ',\\tO',\n",
       " 'because\\tO',\n",
       " 'the\\tO',\n",
       " 'ios\\tB-Object',\n",
       " 'code\\tO',\n",
       " 'supported\\tO',\n",
       " 'ibeacons\\tO',\n",
       " 'earlier\\tB-Predicate',\n",
       " 'than\\tO',\n",
       " 'android\\tB-Object',\n",
       " ',\\tO',\n",
       " 'but\\tO',\n",
       " 'we\\tO',\n",
       " 'are\\tO',\n",
       " 'almost\\tO',\n",
       " 'finished\\tO',\n",
       " 'with\\tO',\n",
       " 'an\\tO',\n",
       " 'android\\tB-Object',\n",
       " 'one\\tO',\n",
       " 'as\\tO',\n",
       " 'well\\tO',\n",
       " '.\\tO']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_Artem.tsv', 'w') as f:\n",
    "    for pred in all_preds:\n",
    "        f.write('\\n'.join([f'{w}\\t{t}' for w, t in zip(pred['words'], pred['tags'])]))\n",
    "        \n",
    "        if i < len(all_preds) - 1:\n",
    "            f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_test.tsv', 'w') as f:\n",
    "    for i, pred in enumerate(all_preds):\n",
    "        f.write('\\n'.join([f'{w}\\t{t}' for w, t in zip(pred['words'], pred['tags'])]))\n",
    "        \n",
    "        if i < len(all_preds) - 1:\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
