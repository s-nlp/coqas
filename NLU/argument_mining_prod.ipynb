{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f60d01e7930>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 28 12:09:02 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 68%   67C    P2   255W / 260W |  10586MiB / 11019MiB |     95%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 71%   68C    P2   260W / 260W |  10586MiB / 11019MiB |     93%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   36C    P8     5W / 260W |   4468MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8    11W / 260W |      9MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 27%   26C    P8     1W / 260W |   3770MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  Off  | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 35%   47C    P2    66W / 260W |    898MiB / 11019MiB |     14%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  Off  | 00000000:40:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8    10W / 260W |   3150MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  Off  | 00000000:41:00.0 Off |                  N/A |\n",
      "| 27%   27C    P8    15W / 260W |   5422MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "cuda_device = torch.device('cuda:2')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "for i in range(n_gpu):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import PretrainedTransformerMismatchedIndexer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "#BERT_MODEL = 'bert-base-cased'\n",
    "BERT_MODEL = 'google/electra-base-discriminator'\n",
    "indexer = PretrainedTransformerMismatchedIndexer(model_name=BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Sequence, Iterable\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.dataset_readers.dataset_utils import to_bioul\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, Field, MetadataField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _is_divider(line: str) -> bool:\n",
    "    empty_line = line.strip() == \"\"\n",
    "    if empty_line:\n",
    "        return True\n",
    "    else:\n",
    "        first_token = line.split()[0]\n",
    "        if first_token == \"-DOCSTART-\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers import Conll2003DatasetReader\n",
    "from allennlp.data.dataset_readers import SequenceTaggingDatasetReader\n",
    "\n",
    "reader = Conll2003DatasetReader(token_indexers={'tokens': indexer})\n",
    "#reader = ConllUniversalReader(token_indexers={'tokens': indexer})\n",
    "#train_dataset = reader.read('train.tsv')\n",
    "#dev_dataset = reader.read('dev.tsv')\n",
    "#test_dataset = reader.read('data_1/test_no_answers.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4fd5fd409946a9a5ba79ba1df9d243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/2334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset.instances)\n",
    "train_dataset.index_with(vocab)\n",
    "dev_dataset.index_with(vocab)\n",
    "#test_dataset.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import PretrainedTransformerMismatchedEmbedder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.seq2seq_encoders import PassThroughEncoder\n",
    "\n",
    "\n",
    "embedder = PretrainedTransformerMismatchedEmbedder(model_name=BERT_MODEL)\n",
    "text_field_embedder = BasicTextFieldEmbedder({'tokens': embedder})\n",
    "seq2seq_encoder = PassThroughEncoder(input_dim=embedder.get_output_dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import SimpleTagger\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "vocab= Vocabulary.from_files(\"./vocab_dir\")\n",
    "\n",
    "model = SimpleTagger(text_field_embedder=text_field_embedder, \n",
    "                      vocab=vocab, \n",
    "                      encoder=seq2seq_encoder,\n",
    "                      calculate_span_f1=True,\n",
    "                      label_encoding='IOB1').cuda(device=cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"roberta.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allennlp.models.simple_tagger.SimpleTagger"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Sequence, Iterable\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "from overrides import overrides\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.instance import Instance\n",
    "\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, Field, MetadataField\n",
    "\n",
    "def _is_divider(line: str) -> bool:\n",
    "    empty_line = line.strip() == \"\"\n",
    "    if empty_line:\n",
    "        return True\n",
    "    else:\n",
    "        first_token = line.split()[0]\n",
    "        if first_token == \"-DOCSTART-\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "class ConllUniversalReader(DatasetReader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_indexers: Dict[str, TokenIndexer] = None,\n",
    "        tag_index: int = 0,\n",
    "        coding_scheme: str = \"IOB1\",\n",
    "        label_namespace: str = \"labels\",\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        \n",
    "        if coding_scheme not in (\"IOB1\", \"BIOUL\"):\n",
    "            raise ConfigurationError(\"unknown coding_scheme: {}\".format(coding_scheme))\n",
    "\n",
    "        self.tag_index = tag_index\n",
    "        self.coding_scheme = coding_scheme\n",
    "        self.label_namespace = label_namespace\n",
    "        self._original_coding_scheme = \"IOB1\"\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        # if `file_path` is a URL, redirect to the cache\n",
    "        file_path = cached_path(file_path)\n",
    "\n",
    "        with open(file_path, \"r\") as data_file:\n",
    "            logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "\n",
    "            # Group into alternative divider / sentence chunks.\n",
    "            for is_divider, lines in itertools.groupby(data_file, _is_divider):\n",
    "                # Ignore the divider chunks, so that `lines` corresponds to the words\n",
    "                # of a single sentence.\n",
    "                if not is_divider:\n",
    "                    fields = [line.strip().split() for line in lines]\n",
    "                    # unzipping trick returns tuples, but our Fields need lists\n",
    "                    fields = [list(field) for field in zip(*fields)]\n",
    "                    tokens_ = fields[0]\n",
    "                    if self.tag_index >= 0:\n",
    "                        ner_tags = fields[1:][self.tag_index]\n",
    "                    else:\n",
    "                        ner_tags = None\n",
    "                    # TextField requires `Token` objects\n",
    "                    tokens = [Token(token) for token in tokens_]\n",
    "\n",
    "                    yield self.text_to_instance(tokens, ner_tags)\n",
    "\n",
    "    def text_to_instance(  # type: ignore\n",
    "        self,\n",
    "        tokens: List[Token],\n",
    "        ner_tags: List[str] = None,\n",
    "    ) -> Instance:\n",
    "        \"\"\"\n",
    "        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n",
    "        \"\"\"\n",
    "\n",
    "        sequence = TextField(tokens, self._token_indexers)\n",
    "        instance_fields: Dict[str, Field] = {\"tokens\": sequence}\n",
    "        instance_fields[\"metadata\"] = MetadataField({\"words\": [x.text for x in tokens]})\n",
    "\n",
    "        # Recode the labels if necessary.\n",
    "        if self.coding_scheme == \"BIOUL\":\n",
    "            coded_ner = (\n",
    "                to_bioul(ner_tags, encoding=self._original_coding_scheme)\n",
    "                if ner_tags is not None\n",
    "                else None\n",
    "            )\n",
    "        else:\n",
    "            # the default IOB1\n",
    "            coded_ner = ner_tags\n",
    "\n",
    "        \n",
    "        # Add \"tag label\" to instance\n",
    "        if coded_ner:\n",
    "            instance_fields[\"tags\"] = SequenceLabelField(coded_ner, sequence, self.label_namespace)\n",
    "        \n",
    "        return Instance(instance_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ConllUniversalReader(token_indexers={'tokens': indexer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'B-Object'),\n",
       " ('is', 'O'),\n",
       " ('better', 'B-Predicate'),\n",
       " ('than', 'O'),\n",
       " ('C++', 'B-Object'),\n",
       " ('for', 'O'),\n",
       " ('developers', 'O')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "predictor = SentenceTaggerPredictor(model, reader)\n",
    "preds = predictor.predict('Python is better than C++ for developers')\n",
    "list(zip(preds['words'], preds['tags']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-Object', 'O', 'B-Predicate', 'O', 'B-Object']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"/notebook/NLU_last_version/models/model_warm_up.th\", 'wb') as f:\n",
    "    #torch.save(model.state_dict(), f)\n",
    "\n",
    "#vocab.save_to_files(\"/notebook/NLU_last_version/models/vocabulary_warm_up\")\n",
    "\n",
    "#vocab2 = Vocabulary.from_files(\"/tmp/vocabulary\")\n",
    "\n",
    "#model2 = LstmTagger(word_embeddings, lstm, vocab2)\n",
    "#with open(\"/tmp/model.th\", 'rb') as f:\n",
    "    #model2.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval on the dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_labels = [list(e['tags']) for e in dev_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n"
     ]
    }
   ],
   "source": [
    "predictor = CustomSentenceTaggerPredictor(model, reader)\n",
    "\n",
    "dev_eval_loader = DataLoader(dataset=dev_dataset, batch_size=100, \n",
    "                             shuffle=False, collate_fn=lambda a: a)\n",
    "\n",
    "all_preds = []\n",
    "for batch in dev_eval_loader:\n",
    "    all_preds += predictor.predict_batch_instance(batch)\n",
    "    \n",
    "pred_tags = [pred['tags'][:len(pred['words'])] for pred in all_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7181756296800544\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "   Object       0.66      0.71      0.69       781\n",
      "   Aspect       0.55      0.57      0.56       257\n",
      "Predicate       0.87      0.91      0.89       386\n",
      "\n",
      "micro avg       0.70      0.74      0.72      1424\n",
      "macro avg       0.70      0.74      0.72      1424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "print(f1_score(dev_labels, pred_tags))\n",
    "print(classification_report(dev_labels, pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('georgia', 'B-Object'),\n",
       " ('has', 'O'),\n",
       " ('a', 'O'),\n",
       " ('higher', 'B-Predicate'),\n",
       " ('percentage', 'B-Aspect'),\n",
       " ('of', 'I-Aspect'),\n",
       " ('blacks', 'B-Aspect'),\n",
       " ('(', 'O'),\n",
       " ('30', 'O'),\n",
       " ('.', 'O'),\n",
       " ('5', 'O'),\n",
       " (')', 'O'),\n",
       " ('than', 'O'),\n",
       " ('new', 'B-Object'),\n",
       " ('york', 'B-Object'),\n",
       " ('(', 'O'),\n",
       " ('15', 'O'),\n",
       " ('.', 'O'),\n",
       " ('9', 'O'),\n",
       " (')', 'O'),\n",
       " ('or', 'O'),\n",
       " ('california', 'B-Object'),\n",
       " ('(', 'O'),\n",
       " ('6', 'O'),\n",
       " ('.', 'O'),\n",
       " ('2', 'O'),\n",
       " (').', 'O')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_num = 47\n",
    "list(zip(all_preds[sent_num]['words'], all_preds[sent_num]['tags']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0bf0bacf4d4a37a005f2f6583ee2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='reading instances'), FloatProgress(value=1.0, bar_style='info', layout=Layout(widthâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reader = ConllUniversalReader(token_indexers={'tokens': indexer}, tag_index=-1)\n",
    "test_dataset_no_answers = reader.read('test_no_answers.tsv')\n",
    "predict_data_loader = DataLoader(dataset=test_dataset_no_answers, batch_size=100, \n",
    "                                 shuffle=False, collate_fn=lambda a: a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = CustomSentenceTaggerPredictor(model, reader) \n",
    "\n",
    "predict_data_loader = DataLoader(dataset=test_dataset_no_answers, batch_size=100, \n",
    "                                 shuffle=False, collate_fn=lambda a: a)\n",
    "\n",
    "all_preds = []\n",
    "for batch in predict_data_loader:\n",
    "    all_preds += predictor.predict_batch_instance(batch)\n",
    "    \n",
    "pred_tags = [pred['tags'][:len(pred['words'])] for pred in all_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plus', ',', 'android', 'is', 'developing', 'a', 'way', 'faster', 'than', 'ios', 'so', 'it', 'has', 'chances', 'to', 'become', 'a', 'laptop', 'replacement', 'earlier', 'than', 'ios', '.']\n",
      "['went', 'to', 'android', 'earlier', 'this', 'year', 'after', 'being', 'convinced', 'its', 'better', 'then', 'ios', 'apple', '.']\n",
      "['the', 'version', 'we', 'showed', 'here', 'is', 'ios', 'only', ',', 'because', 'the', 'ios', 'code', 'supported', 'ibeacons', 'earlier', 'than', 'android', ',', 'but', 'we', 'are', 'almost', 'finished', 'with', 'an', 'android', 'one', 'as', 'well', '.']\n"
     ]
    }
   ],
   "source": [
    "for pred in all_preds[:3]:\n",
    "    print (pred['words'])\n",
    "    a = [f'{w}\\t{t}' for w, t in zip(pred['words'], pred['tags'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the\\tO',\n",
       " 'version\\tO',\n",
       " 'we\\tO',\n",
       " 'showed\\tO',\n",
       " 'here\\tO',\n",
       " 'is\\tO',\n",
       " 'ios\\tB-Object',\n",
       " 'only\\tO',\n",
       " ',\\tO',\n",
       " 'because\\tO',\n",
       " 'the\\tO',\n",
       " 'ios\\tB-Object',\n",
       " 'code\\tO',\n",
       " 'supported\\tO',\n",
       " 'ibeacons\\tO',\n",
       " 'earlier\\tB-Predicate',\n",
       " 'than\\tO',\n",
       " 'android\\tB-Object',\n",
       " ',\\tO',\n",
       " 'but\\tO',\n",
       " 'we\\tO',\n",
       " 'are\\tO',\n",
       " 'almost\\tO',\n",
       " 'finished\\tO',\n",
       " 'with\\tO',\n",
       " 'an\\tO',\n",
       " 'android\\tB-Object',\n",
       " 'one\\tO',\n",
       " 'as\\tO',\n",
       " 'well\\tO',\n",
       " '.\\tO']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_Artem.tsv', 'w') as f:\n",
    "    for pred in all_preds:\n",
    "        f.write('\\n'.join([f'{w}\\t{t}' for w, t in zip(pred['words'], pred['tags'])]))\n",
    "        \n",
    "        if i < len(all_preds) - 1:\n",
    "            f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_test.tsv', 'w') as f:\n",
    "    for i, pred in enumerate(all_preds):\n",
    "        f.write('\\n'.join([f'{w}\\t{t}' for w, t in zip(pred['words'], pred['tags'])]))\n",
    "        \n",
    "        if i < len(all_preds) - 1:\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extractorRoberta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
