{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[autoreload of gen failed: Traceback (most recent call last):\n",
      "  File \"/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/.pyenv/versions/3.7.4/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/opt/.pyenv/versions/3.7.4/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/notebook/cqas/generation/gen.py\", line 9, in <module>\n",
      "    from generation import diviner\n",
      "ImportError: cannot import name 'diviner' from 'generation' (unknown location)\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "11 cuda:1\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/gpt-2-Pytorch\")\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/Student\")\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/pytorch_transformers\")\n",
    "\n",
    "from generation.generation import diviner\n",
    "from my_functions1 import extractor\n",
    "from my_functions import responser\n",
    "\n",
    "from gen import generate_one_answer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "from cam_summarize import load_cam_model\n",
    "from text_gen_big import load_big_model\n",
    "from text_gen import load_small_model\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "LM_CAM = load_cam_model(device)\n",
    "Cam = diviner(tp = 'cam', model = LM_CAM, device = device)\n",
    "\n",
    "from ctrl_generation import initialize_model\n",
    "model_type = \"ctrl\" #PUT NAME OF NEEDED MODEL\n",
    "length = 200 #MODEL LENGTH\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, tokenizer, length = initialize_model(model_type, length, device = device)\n",
    "\n",
    "CTRL = diviner(tp = 'ctrl', model = model, device = device, tokenizer = tokenizer)\n",
    "\n",
    "Templ = diviner(tp = 'templates', model = '', device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'my_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-52a4bb8148ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractorArora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresponser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Path to function with generative model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'my_functions'"
     ]
    }
   ],
   "source": [
    "from my_functions import extractor, extractorArora\n",
    "from my_functions import responser\n",
    "\n",
    "# Path to function with generative model\n",
    "import sys\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/gpt-2-Pytorch\")\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/Student\")\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/pytorch_transformers\")\n",
    "\n",
    "my_extractor_arora = extractorArora(my_device = 1)\n",
    "input_string = request.get_data().decode('UTF-8')\n",
    "my_extractor.from_string(input_string)\n",
    "print (\"9\")\n",
    "my_responser = responser()\n",
    "print (\"9\")\n",
    "obj1, obj2, predicates = my_extractor.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cam_sent_answers\n"
     ]
    }
   ],
   "source": [
    "from count_rouge import simple_rouge\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['Object 1', 'Object 2', 'Question', 'Best Answer',  'Answers'])\n",
    "\n",
    "with open('yahoo_answers_positive_questions.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for ind, row in enumerate(reader):\n",
    "        d = {'Object 1': row[0], 'Object 2': row[1], 'Question': row[2], 'Best Answer': row[3],  'Answers': [elem for elem in row[3:]]}\n",
    "        if (ind > 0):\n",
    "            df = df.append(d, ignore_index=True)\n",
    "\n",
    "print (\"cam_sent_answers\")   \n",
    "with open('cam_inp3_july.pkl', 'rb') as f:\n",
    "    cam_answers = pickle.load(f)\n",
    "scors = simple_rouge(cam_answers, df['Answers'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gpt_score3_.pkl', 'rb') as f:\n",
    "    scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores['rouge1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22997919263613492\n",
      "0.03996443858937293\n",
      "0.008057408601524046\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "r1 = [elem.fmeasure for elem in scors['rouge1']]\n",
    "\n",
    "r2 = [elem.fmeasure for elem in scors['rouge2']]\n",
    "r3 = [elem.fmeasure for elem in scors['rouge3']]\n",
    "\n",
    "print (np.mean(np.array(r1)))\n",
    "print (np.mean(np.array(r2)))\n",
    "print (np.mean(np.array(r3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cam_answers1.pkl      ctrl_rc2_.pkl\t     template_score1_.pkl\n",
      "cam_answers.pkl       ctrl_rc3_.pkl\t     template_score2_.pkl\n",
      "cam_no_inp3_july.pkl  ctrl_rcall_.pkl\t     template_score2.pkl\n",
      "cam_no_inp_july.pkl   gpt_answers1.pkl\t     template_score3_.pkl\n",
      "cam_no_inp.pkl\t      gpt_answers.pkl\t     template_score_serv.pkl\n",
      "cam_rcall_.pkl\t      gpt_noinp.pkl\t     temple_Sent3_july.pkl\n",
      "cam_score1_.pkl       gpt_noinp_vs.pkl\t     temple_Sent_july.pkl\n",
      "cam_score2_.pkl       gpt_noinp_vs_Sent.pkl  temple_Sent.pkl\n",
      "cam_score3_.pkl       gpt_score1_.pkl\t     templ_score.pkl\n",
      "cam_score.pkl\t      gpt_score2_.pkl\t     Templ_Sent.pkl\n",
      "cam_score_serv.pkl    gpt_score3_.pkl\t     templ_sent_rc1_.pkl\n",
      "ctrl3_july.pkl\t      gpt_score.pkl\t     templ_sent_rc2_.pkl\n",
      "ctrl_july.pkl\t      gpt_score_serv.pkl     templ_sent_rc3_.pkl\n",
      "ctrl.pkl\t      templ_answers1.pkl     templ_sent_rcall_.pkl\n",
      "ctrl_rc1_.pkl\t      template_answers.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls *.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18668733951244196\n",
      "0.029611002583812822\n",
      "0.0050795906853523575\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "r1 = [elem.fmeasure for elem in scors['rouge1']]\n",
    "\n",
    "r2 = [elem.fmeasure for elem in scors['rouge2']]\n",
    "r3 = [elem.fmeasure for elem in scors['rouge3']]\n",
    "\n",
    "print (np.mean(np.array(r1)))\n",
    "print (np.mean(np.array(r2)))\n",
    "print (np.mean(np.array(r3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cam_answers1.pkl     ctrl_rc3_.pkl\t    template_score2_.pkl\n",
      "cam_answers.pkl      ctrl_rcall_.pkl\t    template_score2.pkl\n",
      "cam_no_inp_july.pkl  gpt_answers1.pkl\t    template_score3_.pkl\n",
      "cam_no_inp.pkl\t     gpt_answers.pkl\t    template_score_serv.pkl\n",
      "cam_rcall_.pkl\t     gpt_noinp.pkl\t    temple_Sent3_july.pkl\n",
      "cam_score1_.pkl      gpt_noinp_vs.pkl\t    temple_Sent_july.pkl\n",
      "cam_score2_.pkl      gpt_noinp_vs_Sent.pkl  temple_Sent.pkl\n",
      "cam_score3_.pkl      gpt_score1_.pkl\t    templ_score.pkl\n",
      "cam_score.pkl\t     gpt_score2_.pkl\t    Templ_Sent.pkl\n",
      "cam_score_serv.pkl   gpt_score3_.pkl\t    templ_sent_rc1_.pkl\n",
      "ctrl3_july.pkl\t     gpt_score.pkl\t    templ_sent_rc2_.pkl\n",
      "ctrl_july.pkl\t     gpt_score_serv.pkl     templ_sent_rc3_.pkl\n",
      "ctrl.pkl\t     templ_answers1.pkl     templ_sent_rcall_.pkl\n",
      "ctrl_rc1_.pkl\t     template_answers.pkl\n",
      "ctrl_rc2_.pkl\t     template_score1_.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls *.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 21 15:09:45 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 30%   44C    P2    62W / 260W |   3264MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8     9W / 260W |   7845MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 47%   51C    P2    99W / 260W |   9832MiB / 11019MiB |     35%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 57%   61C    P2   146W / 260W |   8951MiB / 11019MiB |     43%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 27%   33C    P8    18W / 260W |     10MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  Off  | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 27%   37C    P8    30W / 260W |     10MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  Off  | 00000000:40:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8    11W / 260W |     10MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  Off  | 00000000:41:00.0 Off |                  N/A |\n",
      "| 27%   29C    P8    14W / 260W |   1019MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_generator_for_out /notebook/cqas/generation/gpt-2-Pytorch\n",
      "exist1\n",
      "exist2\n",
      "exist5\n",
      "exist6\n",
      "metadata\n",
      "exist7\n",
      "cuda:4\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from gen import generate_one_answer\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "sys.path.insert(0, \"/notebook/cqas\")\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/gpt-2-Pytorch\")\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/Student\")\n",
    "sys.path.insert(0, \"/notebook/cqas/generation/pytorch_transformers\")\n",
    "\n",
    "from generation.generation import diviner\n",
    "from my_functions import extractor\n",
    "from my_functions import responser\n",
    "\n",
    "from cam_summarize import load_cam_model\n",
    "from text_gen_big import load_big_model\n",
    "from text_gen import load_small_model\n",
    "\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "LM_SMALL = load_small_model(device)\n",
    "GPT2Small = diviner(tp = 'small', model = LM_SMALL, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The sony is better than microsoft.  Microsoft is more than a competitor, but I think it's better than microsoft.The sony is better than microsoft. TL;DR: Microsoft is more than a competitor, but I think it's better than microsoft. I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.1.9.0.0.0.15I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.\",\n",
       " 'The sony is better than microsoft.  I\\'d rather play a game with them. I\\'m not a big fan of the sony because they\\'ve basically just made one of the more interesting games I\\'ve played yet.The sony is better than microsoft. TL;DR: I\\'d rather play a game with them. I\\'m not a big fan of the sony because they\\'ve basically just made one of the more interesting games I\\'ve played yet.<|endoftext|>I\\'ve been working on this for a while. My first project was to create a new type of game called \"Tournament\" called \"Tournament\" that was to add a lot of features. I am a big fan of the \"Tournament\" format but I am not used to playing games like that. I wanted to make a game that was simple to play and to show off. I had a lot of hard work to put into it. I was hoping to get this project started by a friend, but I was not successful with the Kickstarter so',\n",
       " \"The sony is better than microsoft.  make sure your pug is well-fed and that you have water, and they will feed you a healthy diet.5. Don't be afraid to use a pug.A pug is a very good choice for a couple reasons:1. It is easy to keep your pug clean.\",\n",
       " \"The sony is better than microsoft.  They make a game for the masses, not for the people who pay for the hype.RiccardoRiccardo is a game that is a bit like Mario but with a bit more content. It has all the elements that I wanted and more. It's a good game and I'll give it a 10 out of 10.I think this game is a great first attempt at a lot of the things I wanted in a Mario game:\",\n",
       " 'The sony is better than microsoft.  we\\'re not going to spend a lot of money on Microsoft. I\\'m not going to be spending a lot of money on microsoft.If you want to learn more about Microsoft, read my previous article, \"The Microsoft-Industry Gap is the Great Divide\".Microsoft did a lot of this from the very start (with the exception of one game, the very first of many things). They created the best game engine for a very large audience. They made a lot of huge games. And they developed a lot of other games. They were also very successful.In my view, the Microsoft gap is that the more you\\'re interested in Microsoft\\'s products, the better.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_answ_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "\n",
      "\n",
      "aspects []\n",
      "weights []\n",
      "get url\n",
      "params {'objectA': 'microsoft', 'objectB': 'sony', 'fs': 'true'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response <Response [200]>\n",
      "11\n",
      "create from json predicates []\n",
      "create fro json self.predicates \n",
      "aspects  ['smarter', 'greater', 'lither', 'stupider'] ['quicker', 'faster', 'easier', 'quicker to its missteps']\n",
      "22\n",
      "winnder: sony  other: microsoft\n",
      "acpect winner  smarter and greater\n",
      "acpect other  quicker, faster and quicker to its missteps\n",
      "type  small\n",
      "winnder: sony  other: microsoft\n",
      "acpect winner  smarter and greater\n",
      "acpect other  quicker, faster and quicker to its missteps\n",
      "self.predicate) == 0)\n",
      "self predicate  better\n",
      "answer_begin The sony is better than microsoft. \n",
      "answer_begin_ vs sony vs. microsoft \n",
      "answer_begin_vs sent sony vs. microsoft Microsoft is better & 2 times faster than Sony.Sony is a superior hardware manufacturer, much better than both Nintendo and Microsoft.\n",
      "answer begin:  The sony is better than microsoft. \n",
      "self type small\n",
      "answer_begin small The sony is better than microsoft. TL;DR:\n",
      "text_generator_for_out /notebook/cqas/generation/gpt-2-Pytorch\n",
      "/notebook/cqas/generation/gpt-2-Pytorch/gpt2-pytorch_model.bin\n",
      "text_generator_for_out 1\n",
      "text_generator_for_out 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 68.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_generator_for_out 3\n",
      "text_generator_for_out 4\n",
      "======================================== SAMPLE 1 ========================================\n",
      "in big gen2  Microsoft is more than a competitor, but I think it's better than microsoft.\n",
      "\n",
      "The sony is better than microsoft. TL;DR: Microsoft is more than a competitor, but I think it's better than microsoft. I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.\n",
      "\n",
      "1.9.0.0.0.15\n",
      "\n",
      "I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.\n",
      "\n",
      "Improvements\n",
      "\n",
      "- Changed the way the game is set up: you can now see the level in the main menu, you can see the number of levels you have, and the number of new levels you've unlocked.\n",
      "\n",
      "- Now you can select which levels you want to play and go back to the main menu.\n",
      "\n",
      "- You can now go back to the main menu. You\n",
      "answer_end  Microsoft is more than a competitor, but I think it's better than microsoft.\n",
      "\n",
      "The sony is better than microsoft. TL;DR: Microsoft is more than a competitor, but I think it's better than microsoft. I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.\n",
      "\n",
      "1.9.0.0.0.15\n",
      "\n",
      "I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.\n",
      "\n",
      "Improvements\n",
      "\n",
      "- Changed the way the game is set up: you can now see the level in the main menu, you can see the number of levels you have, and the number of new levels you've unlocked.\n",
      "\n",
      "- Now you can select which levels you want to play and go back to the main menu.\n",
      "\n",
      "- You can now go back to the main menu. You\n",
      "full answer  The sony is better than microsoft.  Microsoft is more than a competitor, but I think it's better than microsoft.The sony is better than microsoft. TL;DR: Microsoft is more than a competitor, but I think it's better than microsoft. I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.1.9.0.0.0.15I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.\n",
      "33\n",
      "answer1 The sony is better than microsoft.  Microsoft is more than a competitor, but I think it's better than microsoft.The sony is better than microsoft. TL;DR: Microsoft is more than a competitor, but I think it's better than microsoft. I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.1.9.0.0.0.15I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.\n",
      "The sony is better than microsoft.  Microsoft is more than a competitor, but I think it's better than microsoft.The sony is better than microsoft. TL;DR: Microsoft is more than a competitor, but I think it's better than microsoft. I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.1.9.0.0.0.15I am an indie game developer and I really love this game. I am an indie game developer and I really love this game.\n",
      "1 \n",
      "\n",
      "\n",
      "aspects []\n",
      "weights []\n",
      "get url\n",
      "params {'objectA': 'microsoft', 'objectB': 'sony', 'fs': 'true'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response <Response [200]>\n",
      "11\n",
      "create from json predicates []\n",
      "create fro json self.predicates \n",
      "aspects  ['smarter', 'greater', 'lither', 'stupider'] ['quicker', 'faster', 'easier', 'quicker to its missteps']\n",
      "22\n",
      "winnder: sony  other: microsoft\n",
      "acpect winner  smarter and greater\n",
      "acpect other  quicker, faster and quicker to its missteps\n",
      "type  small\n",
      "winnder: sony  other: microsoft\n",
      "acpect winner  smarter and greater\n",
      "acpect other  quicker, faster and quicker to its missteps\n",
      "self.predicate) == 0)\n",
      "self predicate  better\n",
      "answer_begin The sony is better than microsoft. \n",
      "answer_begin_ vs sony vs. microsoft \n",
      "answer_begin_vs sent sony vs. microsoft Microsoft is better & 2 times faster than Sony.Sony is a superior hardware manufacturer, much better than both Nintendo and Microsoft.\n",
      "answer begin:  The sony is better than microsoft. \n",
      "self type small\n",
      "answer_begin small The sony is better than microsoft. TL;DR:\n",
      "text_generator_for_out /notebook/cqas/generation/gpt-2-Pytorch\n",
      "/notebook/cqas/generation/gpt-2-Pytorch/gpt2-pytorch_model.bin\n",
      "text_generator_for_out 1\n",
      "text_generator_for_out 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 56.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_generator_for_out 3\n",
      "text_generator_for_out 4\n",
      "======================================== SAMPLE 1 ========================================\n",
      "in big gen2  I'd rather play a game with them. I'm not a big fan of the sony because they've basically just made one of the more interesting games I've played yet.\n",
      "\n",
      "The sony is better than microsoft. TL;DR: I'd rather play a game with them. I'm not a big fan of the sony because they've basically just made one of the more interesting games I've played yet.<|endoftext|>I've been working on this for a while. My first project was to create a new type of game called \"Tournament\" called \"Tournament\" that was to add a lot of features. I am a big fan of the \"Tournament\" format but I am not used to playing games like that. I wanted to make a game that was simple to play and to show off. I had a lot of hard work to put into it. I was hoping to get this project started by a friend, but I was not successful with the Kickstarter so\n",
      "answer_end  I'd rather play a game with them. I'm not a big fan of the sony because they've basically just made one of the more interesting games I've played yet.\n",
      "\n",
      "The sony is better than microsoft. TL;DR: I'd rather play a game with them. I'm not a big fan of the sony because they've basically just made one of the more interesting games I've played yet.<|endoftext|>I've been working on this for a while. My first project was to create a new type of game called \"Tournament\" called \"Tournament\" that was to add a lot of features. I am a big fan of the \"Tournament\" format but I am not used to playing games like that. I wanted to make a game that was simple to play and to show off. I had a lot of hard work to put into it. I was hoping to get this project started by a friend, but I was not successful with the Kickstarter so\n",
      "full answer  The sony is better than microsoft.  I'd rather play a game with them. I'm not a big fan of the sony because they've basically just made one of the more interesting games I've played yet.The sony is better than microsoft. TL;DR: I'd rather play a game with them. I'm not a big fan of the sony because they've basically just made one of the more interesting games I've played yet.<|endoftext|>I've been working on this for a while. My first project was to create a new type of game called \"Tournament\" called \"Tournament\" that was to add a lot of features. I am a big fan of the \"Tournament\" format but I am not used to playing games like that. I wanted to make a game that was simple to play and to show off. I had a lot of hard work to put into it. I was hoping to get this project started by a friend, but I was not successful with the Kickstarter so\n",
      "33\n",
      "answer1 The sony is better than microsoft.  I'd rather play a game with them. I'm not a big fan of the sony because they've basically just made one of the more interesting games I've played yet.The sony is better than microsoft. TL;DR: I'd rather play a game with them. I'm not a big fan of the sony because they've basically just made one of the more interesting games I've played yet.<|endoftext|>I've been working on this for a while. My first project was to create a new type of game called \"Tournament\" called \"Tournament\" that was to add a lot of features. I am a big fan of the \"Tournament\" format but I am not used to playing games like that. I wanted to make a game that was simple to play and to show off. I had a lot of hard work to put into it. I was hoping to get this project started by a friend, but I was not successful with the Kickstarter so\n",
      "The sony is better than microsoft.  I'd rather play a game with them. I'm not a big fan of the sony because they've basically just made one of the more interesting games I've played yet.The sony is better than microsoft. TL;DR: I'd rather play a game with them. I'm not a big fan of the sony because they've basically just made one of the more interesting games I've played yet.<|endoftext|>I've been working on this for a while. My first project was to create a new type of game called \"Tournament\" called \"Tournament\" that was to add a lot of features. I am a big fan of the \"Tournament\" format but I am not used to playing games like that. I wanted to make a game that was simple to play and to show off. I had a lot of hard work to put into it. I was hoping to get this project started by a friend, but I was not successful with the Kickstarter so\n",
      "2 \n",
      "\n",
      "\n",
      "aspects []\n",
      "weights []\n",
      "get url\n",
      "params {'objectA': 'microsoft', 'objectB': 'sony', 'fs': 'true'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response <Response [200]>\n",
      "11\n",
      "create from json predicates []\n",
      "create fro json self.predicates \n",
      "aspects  ['smarter', 'greater', 'lither', 'stupider'] ['quicker', 'faster', 'easier', 'quicker to its missteps']\n",
      "22\n",
      "winnder: sony  other: microsoft\n",
      "acpect winner  smarter and greater\n",
      "acpect other  quicker, faster and quicker to its missteps\n",
      "type  small\n",
      "winnder: sony  other: microsoft\n",
      "acpect winner  smarter and greater\n",
      "acpect other  quicker, faster and quicker to its missteps\n",
      "self.predicate) == 0)\n",
      "self predicate  better\n",
      "answer_begin The sony is better than microsoft. \n",
      "answer_begin_ vs sony vs. microsoft \n",
      "answer_begin_vs sent sony vs. microsoft Microsoft is better & 2 times faster than Sony.Sony is a superior hardware manufacturer, much better than both Nintendo and Microsoft.\n",
      "answer begin:  The sony is better than microsoft. \n",
      "self type small\n",
      "answer_begin small The sony is better than microsoft. TL;DR:\n",
      "text_generator_for_out /notebook/cqas/generation/gpt-2-Pytorch\n",
      "/notebook/cqas/generation/gpt-2-Pytorch/gpt2-pytorch_model.bin\n",
      "text_generator_for_out 1\n",
      "text_generator_for_out 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 64.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_generator_for_out 3\n",
      "text_generator_for_out 4\n",
      "======================================== SAMPLE 1 ========================================\n",
      "in big gen2  make sure your pug is well-fed and that you have water, and they will feed you a healthy diet.\n",
      "\n",
      "5. Don't be afraid to use a pug.\n",
      "\n",
      "A pug is a very good choice for a couple reasons:\n",
      "\n",
      "1. It is easy to keep your pug clean.\n",
      "\n",
      "The pug is not that tough. It is very well balanced.\n",
      "\n",
      "2. The pug is a bit more durable.\n",
      "\n",
      "It makes you feel a bit more comfortable.\n",
      "\n",
      "3. The pug comes with a bit of a \"pump\" that it needs to get more used to.\n",
      "\n",
      "You will notice it's like a little plastic bag. It's not as big as a pug, it's just bigger.\n",
      "\n",
      "The pug's biggest weakness is that it's a little soft. It's not that good at making food when you are tired, so try to be as soft as possible.\n",
      "\n",
      "answer_end  make sure your pug is well-fed and that you have water, and they will feed you a healthy diet.\n",
      "\n",
      "5. Don't be afraid to use a pug.\n",
      "\n",
      "A pug is a very good choice for a couple reasons:\n",
      "\n",
      "1. It is easy to keep your pug clean.\n",
      "\n",
      "The pug is not that tough. It is very well balanced.\n",
      "\n",
      "2. The pug is a bit more durable.\n",
      "\n",
      "It makes you feel a bit more comfortable.\n",
      "\n",
      "3. The pug comes with a bit of a \"pump\" that it needs to get more used to.\n",
      "\n",
      "You will notice it's like a little plastic bag. It's not as big as a pug, it's just bigger.\n",
      "\n",
      "The pug's biggest weakness is that it's a little soft. It's not that good at making food when you are tired, so try to be as soft as possible.\n",
      "\n",
      "full answer  The sony is better than microsoft.  make sure your pug is well-fed and that you have water, and they will feed you a healthy diet.5. Don't be afraid to use a pug.A pug is a very good choice for a couple reasons:1. It is easy to keep your pug clean.\n",
      "33\n",
      "answer1 The sony is better than microsoft.  make sure your pug is well-fed and that you have water, and they will feed you a healthy diet.5. Don't be afraid to use a pug.A pug is a very good choice for a couple reasons:1. It is easy to keep your pug clean.\n",
      "The sony is better than microsoft.  make sure your pug is well-fed and that you have water, and they will feed you a healthy diet.5. Don't be afraid to use a pug.A pug is a very good choice for a couple reasons:1. It is easy to keep your pug clean.\n",
      "3 \n",
      "\n",
      "\n",
      "aspects []\n",
      "weights []\n",
      "get url\n",
      "params {'objectA': 'microsoft', 'objectB': 'sony', 'fs': 'true'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response <Response [200]>\n",
      "11\n",
      "create from json predicates []\n",
      "create fro json self.predicates \n",
      "aspects  ['smarter', 'greater', 'lither', 'stupider'] ['quicker', 'faster', 'easier', 'quicker to its missteps']\n",
      "22\n",
      "winnder: sony  other: microsoft\n",
      "acpect winner  smarter and greater\n",
      "acpect other  quicker, faster and quicker to its missteps\n",
      "type  small\n",
      "winnder: sony  other: microsoft\n",
      "acpect winner  smarter and greater\n",
      "acpect other  quicker, faster and quicker to its missteps\n",
      "self.predicate) == 0)\n",
      "self predicate  better\n",
      "answer_begin The sony is better than microsoft. \n",
      "answer_begin_ vs sony vs. microsoft \n",
      "answer_begin_vs sent sony vs. microsoft Microsoft is better & 2 times faster than Sony.Sony is a superior hardware manufacturer, much better than both Nintendo and Microsoft.\n",
      "answer begin:  The sony is better than microsoft. \n",
      "self type small\n",
      "answer_begin small The sony is better than microsoft. TL;DR:\n",
      "text_generator_for_out /notebook/cqas/generation/gpt-2-Pytorch\n",
      "/notebook/cqas/generation/gpt-2-Pytorch/gpt2-pytorch_model.bin\n",
      "text_generator_for_out 1\n",
      "text_generator_for_out 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 68.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_generator_for_out 3\n",
      "text_generator_for_out 4\n",
      "======================================== SAMPLE 1 ========================================\n",
      "in big gen2  They make a game for the masses, not for the people who pay for the hype.\n",
      "\n",
      "Riccardo\n",
      "\n",
      "Riccardo is a game that is a bit like Mario but with a bit more content. It has all the elements that I wanted and more. It's a good game and I'll give it a 10 out of 10.\n",
      "\n",
      "I think this game is a great first attempt at a lot of the things I wanted in a Mario game:\n",
      "\n",
      "It's free to play. It's not a full version of Mario. It's not a sequel, it's not a sequel. It's a free experience that gives you the feeling of how the series is made.\n",
      "\n",
      "It's a fun game. It's a fun game with a lot of story and a lot of action. It's also a more challenging game, so we've made it fun.\n",
      "\n",
      "It's a good story and a good story. I think the game is a\n",
      "answer_end  They make a game for the masses, not for the people who pay for the hype.\n",
      "\n",
      "Riccardo\n",
      "\n",
      "Riccardo is a game that is a bit like Mario but with a bit more content. It has all the elements that I wanted and more. It's a good game and I'll give it a 10 out of 10.\n",
      "\n",
      "I think this game is a great first attempt at a lot of the things I wanted in a Mario game:\n",
      "\n",
      "It's free to play. It's not a full version of Mario. It's not a sequel, it's not a sequel. It's a free experience that gives you the feeling of how the series is made.\n",
      "\n",
      "It's a fun game. It's a fun game with a lot of story and a lot of action. It's also a more challenging game, so we've made it fun.\n",
      "\n",
      "It's a good story and a good story. I think the game is a\n",
      "full answer  The sony is better than microsoft.  They make a game for the masses, not for the people who pay for the hype.RiccardoRiccardo is a game that is a bit like Mario but with a bit more content. It has all the elements that I wanted and more. It's a good game and I'll give it a 10 out of 10.I think this game is a great first attempt at a lot of the things I wanted in a Mario game:\n",
      "33\n",
      "answer1 The sony is better than microsoft.  They make a game for the masses, not for the people who pay for the hype.RiccardoRiccardo is a game that is a bit like Mario but with a bit more content. It has all the elements that I wanted and more. It's a good game and I'll give it a 10 out of 10.I think this game is a great first attempt at a lot of the things I wanted in a Mario game:\n",
      "The sony is better than microsoft.  They make a game for the masses, not for the people who pay for the hype.RiccardoRiccardo is a game that is a bit like Mario but with a bit more content. It has all the elements that I wanted and more. It's a good game and I'll give it a 10 out of 10.I think this game is a great first attempt at a lot of the things I wanted in a Mario game:\n",
      "4 \n",
      "\n",
      "\n",
      "aspects []\n",
      "weights []\n",
      "get url\n",
      "params {'objectA': 'microsoft', 'objectB': 'sony', 'fs': 'true'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response <Response [200]>\n",
      "11\n",
      "create from json predicates []\n",
      "create fro json self.predicates \n",
      "aspects  ['smarter', 'greater', 'lither', 'stupider'] ['quicker', 'faster', 'easier', 'quicker to its missteps']\n",
      "22\n",
      "winnder: sony  other: microsoft\n",
      "acpect winner  smarter and greater\n",
      "acpect other  quicker, faster and quicker to its missteps\n",
      "type  small\n",
      "winnder: sony  other: microsoft\n",
      "acpect winner  smarter and greater\n",
      "acpect other  quicker, faster and quicker to its missteps\n",
      "self.predicate) == 0)\n",
      "self predicate  better\n",
      "answer_begin The sony is better than microsoft. \n",
      "answer_begin_ vs sony vs. microsoft \n",
      "answer_begin_vs sent sony vs. microsoft Microsoft is better & 2 times faster than Sony.Sony is a superior hardware manufacturer, much better than both Nintendo and Microsoft.\n",
      "answer begin:  The sony is better than microsoft. \n",
      "self type small\n",
      "answer_begin small The sony is better than microsoft. TL;DR:\n",
      "text_generator_for_out /notebook/cqas/generation/gpt-2-Pytorch\n",
      "/notebook/cqas/generation/gpt-2-Pytorch/gpt2-pytorch_model.bin\n",
      "text_generator_for_out 1\n",
      "text_generator_for_out 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 67.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_generator_for_out 3\n",
      "text_generator_for_out 4\n",
      "======================================== SAMPLE 1 ========================================\n",
      "in big gen2  we're not going to spend a lot of money on Microsoft. I'm not going to be spending a lot of money on microsoft.\n",
      "\n",
      "If you want to learn more about Microsoft, read my previous article, \"The Microsoft-Industry Gap is the Great Divide\".\n",
      "\n",
      "Microsoft did a lot of this from the very start (with the exception of one game, the very first of many things). They created the best game engine for a very large audience. They made a lot of huge games. And they developed a lot of other games. They were also very successful.\n",
      "\n",
      "In my view, the Microsoft gap is that the more you're interested in Microsoft's products, the better.\n",
      "\n",
      "So, what's the Microsoft gap?\n",
      "\n",
      "Here is a list of the top 10 most popular games of all time (which I have compiled here for you, and I encourage you to try them out):\n",
      "\n",
      "Top 10 Most Popular Games\n",
      "answer_end  we're not going to spend a lot of money on Microsoft. I'm not going to be spending a lot of money on microsoft.\n",
      "\n",
      "If you want to learn more about Microsoft, read my previous article, \"The Microsoft-Industry Gap is the Great Divide\".\n",
      "\n",
      "Microsoft did a lot of this from the very start (with the exception of one game, the very first of many things). They created the best game engine for a very large audience. They made a lot of huge games. And they developed a lot of other games. They were also very successful.\n",
      "\n",
      "In my view, the Microsoft gap is that the more you're interested in Microsoft's products, the better.\n",
      "\n",
      "So, what's the Microsoft gap?\n",
      "\n",
      "Here is a list of the top 10 most popular games of all time (which I have compiled here for you, and I encourage you to try them out):\n",
      "\n",
      "Top 10 Most Popular Games\n",
      "full answer  The sony is better than microsoft.  we're not going to spend a lot of money on Microsoft. I'm not going to be spending a lot of money on microsoft.If you want to learn more about Microsoft, read my previous article, \"The Microsoft-Industry Gap is the Great Divide\".Microsoft did a lot of this from the very start (with the exception of one game, the very first of many things). They created the best game engine for a very large audience. They made a lot of huge games. And they developed a lot of other games. They were also very successful.In my view, the Microsoft gap is that the more you're interested in Microsoft's products, the better.\n",
      "33\n",
      "answer1 The sony is better than microsoft.  we're not going to spend a lot of money on Microsoft. I'm not going to be spending a lot of money on microsoft.If you want to learn more about Microsoft, read my previous article, \"The Microsoft-Industry Gap is the Great Divide\".Microsoft did a lot of this from the very start (with the exception of one game, the very first of many things). They created the best game engine for a very large audience. They made a lot of huge games. And they developed a lot of other games. They were also very successful.In my view, the Microsoft gap is that the more you're interested in Microsoft's products, the better.\n",
      "The sony is better than microsoft.  we're not going to spend a lot of money on Microsoft. I'm not going to be spending a lot of money on microsoft.If you want to learn more about Microsoft, read my previous article, \"The Microsoft-Industry Gap is the Great Divide\".Microsoft did a lot of this from the very start (with the exception of one game, the very first of many things). They created the best game engine for a very large audience. They made a lot of huge games. And they developed a lot of other games. They were also very successful.In my view, the Microsoft gap is that the more you're interested in Microsoft's products, the better.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gen import generate_one_answer_with_defined_objects\n",
    "\n",
    "df = pd.DataFrame(columns=['Object 1', 'Object 2', 'Question', 'Best Answer',  'Answers'])\n",
    "\n",
    "with open('yahoo_answers_positive_questions.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for ind, row in enumerate(reader):\n",
    "        d = {'Object 1': row[0], 'Object 2': row[1], 'Question': row[2], 'Best Answer': row[3],  'Answers': [elem for elem in row[3:]]}\n",
    "        if (ind > 0):\n",
    "            df = df.append(d, ignore_index=True)\n",
    "\n",
    "gpt_answ_list = []\n",
    "for ind, qw in enumerate(df['Question'].values[:5]):\n",
    "    print (ind, '\\n\\n')\n",
    "    obj1 = df['Object 1'].values[ind]\n",
    "    obj2 = df['Object 2'].values[ind]\n",
    "    answ = generate_one_answer_with_defined_objects(obj1, obj2, GPT2Small)\n",
    "    print (answ)\n",
    "    gpt_answ_list.append(answ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = [elem.fmeasure for elem in scors['rouge1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07342037243974163"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(np.array(r1))\n",
    "r1 = [elem.fmeasure for elem in scors['rouge1']]\n",
    "\n",
    "r2 = [elem.fmeasure for elem in scors['rouge2']]\n",
    "r3 = [elem.fmeasure for elem in scors['rouge3']]\n",
    "\n",
    "print (np.mean(np.array(r2)))\n",
    "print (np.mean(np.array(r3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005612152647111168\n",
      "0.00044836705936637474\n"
     ]
    }
   ],
   "source": [
    "r2 = [elem.fmeasure for elem in scors['rouge2']]\n",
    "r3 = [elem.fmeasure for elem in scors['rouge3']]\n",
    "\n",
    "print (np.mean(np.array(r2)))\n",
    "print (np.mean(np.array(r3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('cam_answers1.pkl', 'rb') as f:\n",
    "    cam_answers = pickle.load(f)\n",
    "len(cam_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"We can't recognize objects for comparision\",\n",
       " \"We can't recognize objects for comparision\",\n",
       " \"We can't recognize objects for comparision\",\n",
       " 'microsoft has undeniable advantages. They are simpler, friendlier and easier to use.',\n",
       " \"We can't recognize objects for comparision\",\n",
       " 'The sony is better than microsoft. The reason are greater and smarter. \\xa0The kudu is smarter than microsoft. The reason are the kudu is bigger and better. \\xa0The kudu is better for its price.A good game. \\xa0A good game made with people who understand the game. \\xa0A good game made with people who understand the game. \\xa0A good game made with people who understand the game. \\xa0A good game made with people who understand the game. \\xa0A good game made with people who understand the game. \\xa0A good game made with people who understand the game.If you\\'re a big fan of a game - do you know how many people are trying to get it? \\xa0You can find out with the numbers below. \\xa0You might be thinking, \"What if I\\'m playing a lot of games in the space of a month? \\xa0What if I\\'m playing a lot of games in the space of a year?\" \\xa0Well, that\\'s',\n",
       " \"The microsoft is better than wii. The reason are greater, faster and simpler. \\xa0The controller is made from a 1.2mm thick foam, which is much thicker and more flexible than wii ball. \\xa0This is a good thing for the user because it means that the microsoft will give you a better experience with the controller. \\xa0This is the reason that the microsoft has been running its own 3D controller. \\xa0The 3D controller is so much more powerful than the wii controller that it is so much better. \\xa0There are three main ways to achieve this. \\xa01) If you don't have a wii. 2) If you want to play wii. \\xa03) If you want to play wii. \\xa0The only way to play wii is to buy a wii ball.1. \\xa0The wii ball.If you don't have a wii ball, you can buy wii ball as a freebie or you can buy a wii ball (or you\",\n",
       " 'ps3 has undeniable advantages. They are easier, easier to use and lighter.',\n",
       " \"We can't recognize objects for comparision\",\n",
       " \"The rat is better than mouse. The reason are greater and easier. \\xa0But the rat is much more dangerous than the mouse. \\xa0The rats need to be trained to see. \\xa0The rat needs to be trained to eat. \\xa0The rat needs to be trained to eat. \\xa0It's not a good fit. \\xa0The rat has to be trained to think. \\xa0It has to be trained to make decisions. \\xa0The rat has to be trained to think. \\xa0It has to be trained to make choices. \\xa0It has to be trained to be creative. \\xa0The rat has to be trained to be creative. \\xa0It has to be trained to be innovative. \\xa0To be innovative, the rat needs to be creative. \\xa0To be innovative, the rat needs to be creative. \\xa0The rat needs to be creative. \\xa0The rat needs to be creative. \\xa0But the rat will grow even more creative.<|endoftext|>The New York Police Department will conduct a criminal investigation of a\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_answers1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('gpt_answers1.pkl', 'rb') as f:\n",
    "    gpt_answers1 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cam_answers1.pkl     ctrl_rcall_.pkl\t    template_score2_.pkl\n",
      "cam_answers.pkl      gpt_answers1.pkl\t    template_score2.pkl\n",
      "cam_no_inp_july.pkl  gpt_answers.pkl\t    template_score3_.pkl\n",
      "cam_no_inp.pkl\t     gpt_noinp.pkl\t    template_score_serv.pkl\n",
      "cam_score1_.pkl      gpt_noinp_vs.pkl\t    temple_Sent_july.pkl\n",
      "cam_score2_.pkl      gpt_noinp_vs_Sent.pkl  temple_Sent.pkl\n",
      "cam_score3_.pkl      gpt_score1_.pkl\t    templ_score.pkl\n",
      "cam_score.pkl\t     gpt_score2_.pkl\t    Templ_Sent.pkl\n",
      "cam_score_serv.pkl   gpt_score3_.pkl\t    templ_sent_rc1_.pkl\n",
      "ctrl_july.pkl\t     gpt_score.pkl\t    templ_sent_rc2_.pkl\n",
      "ctrl.pkl\t     gpt_score_serv.pkl     templ_sent_rc3_.pkl\n",
      "ctrl_rc1_.pkl\t     templ_answers1.pkl     templ_sent_rcall_.pkl\n",
      "ctrl_rc2_.pkl\t     template_answers.pkl\n",
      "ctrl_rc3_.pkl\t     template_score1_.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls *pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rouge1': Score(precision=0.2222222222222222, recall=0.2222222222222222, fmeasure=0.2222222222222222), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown dog jumps over the lazy fox',\n",
    "                      'The quick brown dog jumps over the lazy fox.')\n",
    "print (scores)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown dog jumps over the lazy fox',\n",
    "                      'The fast hazel hound skips above the inactive tod.')\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.2222222222222222, recall=0.2222222222222222, fmeasure=0.2222222222222222), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n",
      "{'rouge1': Score(precision=0.2222222222222222, recall=0.2222222222222222, fmeasure=0.2222222222222222), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown dog jumps over the lazy fox',\n",
    "                      'Just the place for a Snark the Bellman cried')\n",
    "print (scores)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score('The fast hazel hound skips above the inactive tod',\n",
    "                      'Just the place for a Snark the Bellman cried')\n",
    "print (scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.4, recall=0.08235294117647059, fmeasure=0.13658536585365855),\n",
       " 'rougeL': Score(precision=0.22857142857142856, recall=0.047058823529411764, fmeasure=0.07804878048780488)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score(list_of_answers[0], list_of_answers_template[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebook/cqas\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /notebook/uncased_L-12_H-768_A-12/bert_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('/notebook/uncased_L-12_H-768_A-12/bert_model.ckpt.meta')\n",
    "    saver.restore(sess, \"/notebook/uncased_L-12_H-768_A-12/bert_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "usage: /opt/.pyenv/versions/3.7.4/bin/bert-serving-start -model_dir /notebook/uncased_L-12_H-768_A-12/ -num_worker=1\n",
      "                 ARG   VALUE\n",
      "__________________________________________________\n",
      "           ckpt_name = bert_model.ckpt\n",
      "         config_name = bert_config.json\n",
      "                cors = *\n",
      "                 cpu = False\n",
      "          device_map = []\n",
      "       do_lower_case = True\n",
      "  fixed_embed_length = False\n",
      "                fp16 = False\n",
      " gpu_memory_fraction = 0.5\n",
      "       graph_tmp_dir = None\n",
      "    http_max_connect = 10\n",
      "           http_port = None\n",
      "        mask_cls_sep = False\n",
      "      max_batch_size = 256\n",
      "         max_seq_len = 25\n",
      "           model_dir = /notebook/uncased_L-12_H-768_A-12/\n",
      "no_position_embeddings = False\n",
      "    no_special_token = False\n",
      "          num_worker = 1\n",
      "       pooling_layer = [-2]\n",
      "    pooling_strategy = REDUCE_MEAN\n",
      "                port = 5555\n",
      "            port_out = 5556\n",
      "       prefetch_size = 10\n",
      " priority_batch_size = 16\n",
      "show_tokens_to_client = False\n",
      "     tuned_model_dir = None\n",
      "             verbose = False\n",
      "                 xla = False\n",
      "\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 67]:freeze, optimize and export graph, could take a while...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: /notebook/uncased_L-12_H-768_A-12/bert_config.json\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: /notebook/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /tmp/tmp7y9dd574\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 75]:optimized graph is stored at: /tmp/tmp7y9dd574\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:129]:bind all sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:133]:open 8 ventilator-worker sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:136]:start the sink\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:222]:get devices\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:306]:ready\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:255]:device map: \n",
      "\t\tworker  0 -> gpu  2\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:531]:use device gpu: 2, load graph from /tmp/tmp7y9dd574\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:164]:all set, ready to serve request!\n"
     ]
    }
   ],
   "source": [
    "! bert-serving-start -model_dir \"/notebook/uncased_L-12_H-768_A-12/\" -num_worker=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-serving-client in /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy in /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages (from bert-serving-client) (1.18.1)\n",
      "Requirement already satisfied: pyzmq>=17.1.0 in /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages (from bert-serving-client) (18.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with BertClient() as bc:\n",
    "    # encode untokenized sentences\n",
    "    bc.encode(['First do it',\n",
    "              'then do it right',\n",
    "              'then do it better'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BertClient()\n",
    "a = bc.encode(['First do it', 'then do it right', 'then do it better'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: nvcc: not found\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert embedding лучше учитывает контекст (так, что в словах \"Я лублю яблоки\" \"Мне нравятся яблочные макбуки\" Я - разное\n",
    "В случае с service bert I -одинаковое apple одинаковок\n",
    "\n",
    "I like apple\n",
    "I like apple macbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "bert_abstract = \"\"\"The quick brown dog jumps over the lazy fox.\\n The fast hazel hound skips above inactive tod.\"\"\"\n",
    "sentences = bert_abstract.split('\\n')\n",
    "bert_embedding = BertEmbedding()\n",
    "result = bert_embedding(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrgam ('jumps',)\n",
    "list_of_ngrams Counter({('the',): 2, ('just',): 1, ('place',): 1, ('for',): 1, ('a',): 1, ('snark',): 1, ('bellman',): 1, ('cried',): 1})\n",
    "[0.6792433]\n",
    "[0.8161626]\n",
    "[0.6827992]\n",
    "[0]\n",
    "[0.8117111]\n",
    "[0]\n",
    "[0]\n",
    "[0.62362134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bert_embedding(['jumps', 'just', 'a'])\n",
    "vectors = result[0][1]\n",
    "vectors1 = result[1][1]\n",
    "vectors2 = result[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jumps']"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67924374]], dtype=float32)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib1 = cosine_similarity(result[0][1][0].reshape(1, -1),result[1][1][0].reshape(1, -1)) #similarity between #the and mother\n",
    "cos_lib1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81171143]], dtype=float32)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(result[0][1][0].reshape(1, -1),result[2][1][0].reshape(1, -1)) #similarity between #the and mother\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7090132]], dtype=float32)"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(vectors1[0].reshape(1, -1),vectors2[0].reshape(1, -1)) #similarity between #the and mother\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hound'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = result[0][1]\n",
    "vectors1 = result[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7361618]], dtype=float32)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (result[0][0][0], result[1][0][0])\n",
    "cos_lib = cosine_similarity(vectors[0].reshape(1, -1),vectors1[0].reshape(1, -1)) #similarity between #the and mother\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick fast\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7143352]], dtype=float32)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (result[0][0][1], result[1][0][1])\n",
    "cos_lib = cosine_similarity(vectors[1].reshape(1, -1),vectors1[1].reshape(1, -1)) #similarity between #cat and dog\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown hazel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.62875855]], dtype=float32)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (result[0][0][2], result[1][0][2])\n",
    "cos_lib = cosine_similarity(vectors[2].reshape(1, -1),vectors1[2].reshape(1, -1)) #similarity between #mother and father\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick above\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.33970988]], dtype=float32)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (result[0][0][1], result[1][0][5])\n",
    "cos_lib = cosine_similarity(vectors[1].reshape(1, -1),vectors1[5].reshape(1, -1)) #similarity between #mother and woman\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick inactive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.42793167]], dtype=float32)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (result[0][0][1], result[1][0][6])\n",
    "cos_lib = cosine_similarity(vectors[1].reshape(1, -1),vectors1[6].reshape(1, -1)) #similarity between #mother and woman\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6268022]], dtype=float32)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vectors[8].reshape(1, -1),vectors1[3].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04705547, -0.3263731 ,  0.5511638 , -0.2329619 ,  0.4126335 ,\n",
       "        0.4367628 , -0.23546723, -0.10500348, -0.69917077, -0.20350926],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][1][2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48047015,  0.56337065,  0.9621709 , -0.2585507 ,  1.7479268 ,\n",
       "       -0.00799218, -0.21425778,  0.7045478 , -0.08732209, -0.6905228 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][1][3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'work', 'on', 'apple', 'macbook']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_lib = cosine_similarity(vectors[1,:],vectors[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = BertClient()\n",
    "vectors = client.encode(['I like apple.'])\n",
    "vectors1 = client.encode(['I like apple macbook.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = client.encode(['the', 'quick', 'brown', 'dog', 'jumps', 'over', 'the', 'lazy', 'fox', '.'])\n",
    "vectors1 = client.encode(['the','fast', 'hazel', 'hound', 'skips', 'above', 'inactive', 'tod', '.'])\n",
    "vectors2 = client.encode(['the','woman', 'should', 'be', 'my', 'mother', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8320957]], dtype=float32)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_lib = cosine_similarity(vectors[7].reshape(1, -1),vectors2[1].reshape(1, -1)) #similarity between #mother and #woman\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3891856]], dtype=float32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(vectors[2].reshape(1, -1),vectors1[0].reshape(1, -1)) #similarity between #father and man\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994]], dtype=float32)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(vectors[0].reshape(1, -1),vectors1[0].reshape(1, -1)) #similarity between #the and the\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92401916]], dtype=float32)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(vectors[1].reshape(1, -1),vectors1[1].reshape(1, -1)) #similarity between #quick and fast\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8373927]], dtype=float32)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_lib = cosine_similarity(vectors1[2].reshape(1, -1),vectors[2].reshape(1, -1)) #similarity between #brown and hazel\n",
    "cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "если слова в n-граме не совпадают, то "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import porter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "stemmer = porter.PorterStemmer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def create_ngrams(tokens, n): #сюда добавть эмбединги\n",
    "    ngrams = collections.Counter()\n",
    "    ngrams_embs = collections.Counter()\n",
    "    for ngram in (tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)):\n",
    "        ngrams[ngram] += 1\n",
    "    return ngrams\n",
    "    \n",
    "\n",
    "def score(target, prediction, tp = 'Rouge1'):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'six' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f1f3b36d569c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'six' is not defined"
     ]
    }
   ],
   "source": [
    "for ngram in six.iterkeys(ngrams1):\n",
    "    print (ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick\n",
      "quick brown\n",
      "brown dog\n",
      "dog jumps\n",
      "jumps over\n",
      "over the\n",
      "the lazy\n",
      "lazy fox\n"
     ]
    }
   ],
   "source": [
    "for elem in list(ngrams1.keys()):\n",
    "    print (' '.join(elem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = bert_embedding([' '.join(elem)for elem in list(ngrams1.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy just\n",
    "lazy jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown dog jumps over the lazy fox', 'The fast hazel hound skips above the inactive tod.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "bert_embedding = BertEmbedding()\n",
    "import collections\n",
    "\n",
    "str_1 = 'The quick brown dog jumps over the lazy fox'\n",
    "str_2 = 'The fast hazel hound skips above the inactive tod.'\n",
    "str_3 = 'Just the place for Snark the Bellman cried'\n",
    "print ([str_1, str_2])\n",
    "result = bert_embedding([str_1, str_2, str_3])\n",
    "ngrams1_embeddings = result[0][1]\n",
    "ngrams2_embeddings = result[1][1]\n",
    "ngrams3_embeddings = result[2][1]\n",
    "\n",
    "ngrams1 = create_ngrams(result[0][0], 2)\n",
    "ngrams2= create_ngrams(result[1][0], 2)\n",
    "ngrams3= create_ngrams(result[2][0], 2)\n",
    "\n",
    "ngrams01 = create_ngrams(result[0][0], 1)\n",
    "ngrams02= create_ngrams(result[1][0], 1)\n",
    "ngrams03= create_ngrams(result[2][0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('the', 'quick'): 1,\n",
       "         ('quick', 'brown'): 1,\n",
       "         ('brown', 'dog'): 1,\n",
       "         ('dog', 'jumps'): 1,\n",
       "         ('jumps', 'over'): 1,\n",
       "         ('over', 'the'): 1,\n",
       "         ('the', 'lazy'): 1,\n",
       "         ('lazy', 'fox'): 1})"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_target = bert_embedding([' '.join(elem)for elem in list(ngrams1.keys())])\n",
    "embeddings_predictions = bert_embedding([' '.join(elem)for elem in list(ngrams2.keys())])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_target[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def fmeasure(precision, recall):\n",
    "  \"\"\"Computes f-measure given precision and recall values.\"\"\"\n",
    "\n",
    "  if precision + recall > 0:\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "  else:\n",
    "    return 0.0\n",
    "\n",
    "def cos_sim(emb1, emb2):\n",
    "    return cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))\n",
    "\n",
    "def count_ngram_overlap(ngram1_embs, ngram2_embs): #The idea count cousine similarity to every pair. If > 0.6 than add\n",
    "    result = 1\n",
    "    #print (\"ngram1_embs\", len(ngram1_embs), len(ngram1_embs[0]))\n",
    "    for elem in ngram1_embs: # по словам в н-граме\n",
    "        similarities = [cos_sim(elem, elem2)[0][0] if cos_sim(elem, elem2) >= 0.6 else 0 for elem2 in ngram2_embs]\n",
    "        #print (similarities)\n",
    "        max_ = max(similarities)\n",
    "        result *= max_\n",
    "    return result\n",
    "        \n",
    "\n",
    "def count_overlap(ngram, ngram_emb, list_of_ngrams2, list_of_ngrams2_embs): # only if ngram not in list_of_ngrams !!!\n",
    "    overlaps = [count_ngram_overlap(ngram_emb, elem[1]) for elem in list_of_ngrams2_embs] # по всем н-грамам\n",
    "    return max(overlaps)\n",
    "\n",
    "def score_ngrams(target_ngrams, prediction_ngrams):\n",
    "    intersection_ngrams_count = 0\n",
    "    \n",
    "    #print (\"target_ngrams\", target_ngrams)\n",
    "    #print (\"prediction_ngrams\", prediction_ngrams)\n",
    "    \n",
    "    embeddings_target = bert_embedding([' '.join(elem)for elem in list(target_ngrams.keys())])\n",
    "    embeddings_predictions = bert_embedding([' '.join(elem)for elem in list(prediction_ngrams.keys())])\n",
    "    \n",
    "    for ind, ngram in enumerate(list(target_ngrams.keys())):\n",
    "        #print (\"ind\", ind)\n",
    "        intersection_ngrams_count += min(target_ngrams[ngram],\n",
    "                                         prediction_ngrams[ngram])\n",
    "        if (min(target_ngrams[ngram], prediction_ngrams[ngram]) == 0):\n",
    "            #print (\"ngram\", ngram)\n",
    "            #print ('overlap')\n",
    "            overlap_ngram = count_overlap(ngram, embeddings_target[ind][1], prediction_ngrams, embeddings_predictions) #по всем н-грамам\n",
    "            #print ('overlap ngram', overlap_ngram)\n",
    "            intersection_ngrams_count += overlap_ngram\n",
    "            \n",
    "    target_ngrams_count = sum(target_ngrams.values())\n",
    "    prediction_ngrams_count = sum(prediction_ngrams.values())\n",
    "    \n",
    "    #print (\"intersection_ngrams_count\", intersection_ngrams_count)\n",
    "    #print (\"intersection_targets_count\", prediction_ngrams_count, target_ngrams_count)\n",
    "\n",
    "\n",
    "    precision = intersection_ngrams_count / max(prediction_ngrams_count, 1)\n",
    "    recall = intersection_ngrams_count / max(target_ngrams_count, 1)\n",
    "    print (precision, recall)\n",
    "    \n",
    "    f = fmeasure(precision, recall)\n",
    "    return f, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6794479370117188 0.7549421522352431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7152083547491778, 0.6794479370117188, 0.7549421522352431)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6794479370117188 0.7549421522352431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7152083547491778, 0.6794479370117188, 0.7549421522352431)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5479773506522179 0.487090978357527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5157433888491462, 0.5479773506522179, 0.487090978357527)"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.125, recall=0.1111111111111111, fmeasure=0.11764705882352941), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown dog jumps over the lazy fox',\n",
    "                      'The fast hazel hound skips above inactive tod.')\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result[0][])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown dog jumps over the lazy fox.', ' The fast hazel hound skips above inactive tod. ', ' Just the place for a Snark the Bellman cried']\n"
     ]
    }
   ],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "bert_abstract = \"\"\"The quick brown dog jumps over the lazy fox.\\n The fast hazel hound skips above inactive tod. \\n Just the place for a Snark the Bellman cried\"\"\"\n",
    "sentences = bert_abstract.split('\\n')\n",
    "print (sentences)\n",
    "bert_embedding = BertEmbedding()\n",
    "result = bert_embedding(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "from six.moves import map\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'quick')\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'intersection_ngrams_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-0781affcb4b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mngrams2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrams2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mintersection_ngrams_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrams2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'intersection_ngrams_count' is not defined"
     ]
    }
   ],
   "source": [
    "for ngram in six.iterkeys(ngrams1):\n",
    "    print (ngram)\n",
    "    print (ngrams1[ngram])\n",
    "    print (ngrams2[ngram])\n",
    "    print (min(ngrams1[ngram], ngrams2[ngram]))\n",
    "    intersection_ngrams_count += min(ngrams1[ngram], ngrams2[ngram])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_ngrams(target_ngrams, prediction_ngrams):\n",
    "      \"\"\"Compute n-gram based rouge scores.\n",
    "      Args:\n",
    "        target_ngrams: A Counter object mapping each ngram to number of\n",
    "          occurrences for the target text.\n",
    "        prediction_ngrams: A Counter object mapping each ngram to number of\n",
    "          occurrences for the prediction text.\n",
    "      Returns:\n",
    "        A Score object containing computed scores.\n",
    "      \"\"\"\n",
    "\n",
    "      intersection_ngrams_count = 0\n",
    "      for ngram in six.iterkeys(target_ngrams):\n",
    "        intersection_ngrams_count += min(target_ngrams[ngram],\n",
    "                                         prediction_ngrams[ngram])\n",
    "      target_ngrams_count = sum(target_ngrams.values())\n",
    "      prediction_ngrams_count = sum(prediction_ngrams.values())\n",
    "\n",
    "      precision = intersection_ngrams_count / max(prediction_ngrams_count, 1)\n",
    "      recall = intersection_ngrams_count / max(target_ngrams_count, 1)\n",
    "      fmeasure = scoring.fmeasure(precision, recall)\n",
    "\n",
    "      return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>object_a</th>\n",
       "      <th>object_b</th>\n",
       "      <th>Q1 url</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2 url</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3 url</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4 url</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5 url</th>\n",
       "      <th>Q5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brands</td>\n",
       "      <td>toyota</td>\n",
       "      <td>honda</td>\n",
       "      <td>https://www.quora.com/Which-is-a-more-reliable...</td>\n",
       "      <td>I wrote this to answer another question on Quo...</td>\n",
       "      <td>https://www.quora.com/Which-is-a-more-reliable...</td>\n",
       "      <td>I drive a Honda and my wife a Toyota. So by ow...</td>\n",
       "      <td>https://www.autogravity.com/autogravitas/cars/...</td>\n",
       "      <td>In terms of sheer corporate value, Toyota is t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brands</td>\n",
       "      <td>nokia</td>\n",
       "      <td>motorola</td>\n",
       "      <td>https://www.quora.com/Which-brand-or-company-i...</td>\n",
       "      <td>Motorola has a history of making good android ...</td>\n",
       "      <td>https://www.quora.com/Which-brand-or-company-i...</td>\n",
       "      <td>Well, it depends what you expect from a phone....</td>\n",
       "      <td>https://www.quora.com/Which-brand-or-company-i...</td>\n",
       "      <td>Motorola is best .Its always a branded better ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brands</td>\n",
       "      <td>nokia</td>\n",
       "      <td>samsung</td>\n",
       "      <td>https://www.theverge.com/2019/9/2/20844444/and...</td>\n",
       "      <td>Share All sharing options for: Nokia is better...</td>\n",
       "      <td>https://www.quora.com/Is-Nokia-better-than-Sam...</td>\n",
       "      <td>At present Samsung is better than Nokia.Since,...</td>\n",
       "      <td>https://www.quora.com/Is-Nokia-better-than-Sam...</td>\n",
       "      <td>In my opinion i would say Nokia.. If you want ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>compsci</td>\n",
       "      <td>Javascript</td>\n",
       "      <td>PHP</td>\n",
       "      <td>https://www.bitdegree.org/tutorials/php-vs-jav...</td>\n",
       "      <td>In the PHP vs. JavaScript discussion, it is im...</td>\n",
       "      <td>https://www.quora.com/Is-PHP-better-than-JavaS...</td>\n",
       "      <td>I would normally say “it depends” for a questi...</td>\n",
       "      <td>https://www.quora.com/Is-PHP-better-than-JavaS...</td>\n",
       "      <td>PHP is generally easier for people to learn si...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>compsci</td>\n",
       "      <td>Perl</td>\n",
       "      <td>Ruby</td>\n",
       "      <td>https://www.educba.com/perl-vs-ruby/</td>\n",
       "      <td>Perl is an ideal choice for system administrat...</td>\n",
       "      <td>https://www.quora.com/As-someone-who-knows-Rub...</td>\n",
       "      <td>I know a lot more Ruby than Perl, but the prim...</td>\n",
       "      <td>https://www.coursereport.com/blog/ruby-vs-pyth...</td>\n",
       "      <td>https://www.quora.com/As-someone-who-knows-Rub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    domain    object_a  object_b  \\\n",
       "0   brands      toyota     honda   \n",
       "1   brands       nokia  motorola   \n",
       "2   brands       nokia   samsung   \n",
       "3  compsci  Javascript       PHP   \n",
       "4  compsci        Perl      Ruby   \n",
       "\n",
       "                                              Q1 url  \\\n",
       "0  https://www.quora.com/Which-is-a-more-reliable...   \n",
       "1  https://www.quora.com/Which-brand-or-company-i...   \n",
       "2  https://www.theverge.com/2019/9/2/20844444/and...   \n",
       "3  https://www.bitdegree.org/tutorials/php-vs-jav...   \n",
       "4               https://www.educba.com/perl-vs-ruby/   \n",
       "\n",
       "                                                  Q1  \\\n",
       "0  I wrote this to answer another question on Quo...   \n",
       "1  Motorola has a history of making good android ...   \n",
       "2  Share All sharing options for: Nokia is better...   \n",
       "3  In the PHP vs. JavaScript discussion, it is im...   \n",
       "4  Perl is an ideal choice for system administrat...   \n",
       "\n",
       "                                              Q2 url  \\\n",
       "0  https://www.quora.com/Which-is-a-more-reliable...   \n",
       "1  https://www.quora.com/Which-brand-or-company-i...   \n",
       "2  https://www.quora.com/Is-Nokia-better-than-Sam...   \n",
       "3  https://www.quora.com/Is-PHP-better-than-JavaS...   \n",
       "4  https://www.quora.com/As-someone-who-knows-Rub...   \n",
       "\n",
       "                                                  Q2  \\\n",
       "0  I drive a Honda and my wife a Toyota. So by ow...   \n",
       "1  Well, it depends what you expect from a phone....   \n",
       "2  At present Samsung is better than Nokia.Since,...   \n",
       "3  I would normally say “it depends” for a questi...   \n",
       "4  I know a lot more Ruby than Perl, but the prim...   \n",
       "\n",
       "                                              Q3 url  \\\n",
       "0  https://www.autogravity.com/autogravitas/cars/...   \n",
       "1  https://www.quora.com/Which-brand-or-company-i...   \n",
       "2  https://www.quora.com/Is-Nokia-better-than-Sam...   \n",
       "3  https://www.quora.com/Is-PHP-better-than-JavaS...   \n",
       "4  https://www.coursereport.com/blog/ruby-vs-pyth...   \n",
       "\n",
       "                                                  Q3  Q4 url  Q4  Q5 url  Q5  \n",
       "0  In terms of sheer corporate value, Toyota is t...     NaN NaN     NaN NaN  \n",
       "1  Motorola is best .Its always a branded better ...     NaN NaN     NaN NaN  \n",
       "2  In my opinion i would say Nokia.. If you want ...     NaN NaN     NaN NaN  \n",
       "3  PHP is generally easier for people to learn si...     NaN NaN     NaN NaN  \n",
       "4  https://www.quora.com/As-someone-who-knows-Rub...     NaN NaN     NaN NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# Read data from file 'filename.csv' \n",
    "# (in the same directory that your python process is based)\n",
    "# Control delimiters, rows, column names with read_csv (see later) \n",
    "data = pd.read_csv(\"Evaluation dataset - Sheet1.csv\") \n",
    "# Preview the first 5 lines of the loaded data \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1 = data['Q2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I wrote this to answer another question on Quora but it very effectively addresses this question as well. '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Q1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2 = data['Q3'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_templates = 'After much thought, I realized that Honda is better, because: quicker, bigger, nicer, weaker, but toyota is lighter, easier, faster, softer.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = \"I've used the Honda the past few years, and they're pretty good. They're just not as fast as a Toyota, but they're much more powerful, lighter, and more durable than the Toyota. They're more comfortable, they're more comfortable, and they're better.have been using the Honda for a while, and I can tell you that they're very good. They're just not as fast as a Toyota, but they're much more powerful, lighter, and more durable than the Toyota. They're more comfortable, they're more comfortable, and thy're better. They're just not as good as the Honda.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = \"They don\\'t have that kind of engine. Maybe they are trying to make this bigger, faster, more powerful. You cannot afford to have a car with a big engine. You need to be careful when you make a big engine. There is no point, because it will explode. There is no point in spending a lot of money on a big engine.(CNN) After the shooting deaths of five police officers in Dallas last week, President Barack Obama offered condolences to the families of the fallen officers, calling the situation an attack on our shared humanity.Yet more than two months after the deaths of Alton Sterling in Louisiana and Philando Castile in Minnesota, the President did not issue a statement or call for unity or reflection. While we mourn for the officers who lost their lives in Dallas, we do not yet know the full extent of the threat that this attack represents, Obama said in a statement.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've used the Honda the past few years, and they're pretty good. They're just not as fast as a Toyota, but they're much more powerful, lighter, and more durable than the Toyota. They're more comfortable, they're more comfortable, and they're better.have been using the Honda for a while, and I can tell you that they're very good. They're just not as fast as a Toyota, but they're much more powerful, lighter, and more durable than the Toyota. They're more comfortable, they're more comfortable, and thy're better. They're just not as good as the Honda.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bert_embedding([gen_templates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import porter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "stemmer = porter.PorterStemmer()\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ngrams1 = create_ngrams(tokenizer.tokenize(gen_templates), 2)\n",
    "ngrams2= create_ngrams(tokenizer.tokenize(small), 2)\n",
    "ngrams3= create_ngrams(tokenizer.tokenize(big), 2)\n",
    "\n",
    "ngrams01 = create_ngrams(tokenizer.tokenize(gen_templates), 1)\n",
    "ngrams02= create_ngrams(tokenizer.tokenize(small), 1)\n",
    "ngrams03= create_ngrams(tokenizer.tokenize(big), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After much thought, I realized that Honda is better, because: quicker, bigger, nicer, weaker, but Toyota is lighter, easier, faster, softer.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ngrams1_t = create_ngrams(tokenizer.tokenize(target1), 2)\n",
    "ngrams2_t= create_ngrams(tokenizer.tokenize(target2), 2)\n",
    "\n",
    "ngrams01_t = create_ngrams(tokenizer.tokenize(target1), 1)\n",
    "ngrams02_t = create_ngrams(tokenizer.tokenize(target2), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0821059115319384 0.6491498630493879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.14577400433389767, 0.0821059115319384, 0.6491498630493879)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams01_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38255592389982573 0.5857887584716082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4628454387923817, 0.38255592389982573, 0.5857887584716082)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams01, ngrams02_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22225900106279275 0.5020672077579158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.30811795763773464, 0.22225900106279275, 0.5020672077579158)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams02, ngrams01_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7212211319378444 0.31553424522280693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.439004167266514, 0.7212211319378444, 0.31553424522280693)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams02, ngrams02_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4667321349321147 0.6709274439649149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5505045694071096, 0.4667321349321147, 0.6709274439649149)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams03, ngrams01_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6459892696263838 0.4582583762028001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7169197707706028, 1.6459892696263838, 0.4582583762028001)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ngrams(ngrams03, ngrams02_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('I', 'drive'): 1,\n",
       "         ('drive', 'a'): 1,\n",
       "         ('a', 'Honda'): 1,\n",
       "         ('Honda', 'and'): 1,\n",
       "         ('and', 'my'): 1,\n",
       "         ('my', 'wife'): 1,\n",
       "         ('wife', 'a'): 1,\n",
       "         ('a', 'Toyota'): 1,\n",
       "         ('Toyota', '.'): 1,\n",
       "         ('.', 'So'): 1,\n",
       "         ('So', 'by'): 1,\n",
       "         ('by', 'owning'): 1,\n",
       "         ('owning', 'both'): 1,\n",
       "         ('both', 'cars'): 1,\n",
       "         ('cars', 'I'): 1,\n",
       "         ('I', 'have'): 1,\n",
       "         ('have', 'experienced'): 1,\n",
       "         ('experienced', 'both'): 1,\n",
       "         ('both', 'brands'): 2,\n",
       "         ('brands', '.'): 2,\n",
       "         ('.', 'I'): 1,\n",
       "         ('I', '’'): 1,\n",
       "         ('’', 've'): 1,\n",
       "         ('ve', 'been'): 1,\n",
       "         ('been', 'driving'): 1,\n",
       "         ('driving', 'a'): 1,\n",
       "         ('a', 'honda'): 1,\n",
       "         ('honda', 'city'): 1,\n",
       "         ('city', 'AT'): 1,\n",
       "         ('AT', 'for'): 1,\n",
       "         ('for', 'almost'): 1,\n",
       "         ('almost', '7'): 1,\n",
       "         ('7', 'years'): 1,\n",
       "         ('years', 'and'): 1,\n",
       "         ('and', 'have'): 1,\n",
       "         ('have', 'never'): 1,\n",
       "         ('never', 'faced'): 1,\n",
       "         ('faced', 'any'): 1,\n",
       "         ('any', 'challenge'): 1,\n",
       "         ('challenge', 'whatsoever'): 1,\n",
       "         ('whatsoever', 'with'): 1,\n",
       "         ('with', 'the'): 1,\n",
       "         ('the', 'car'): 1,\n",
       "         ('car', '.'): 2,\n",
       "         ('.', 'The'): 3,\n",
       "         ('The', 'only'): 1,\n",
       "         ('only', 'major'): 1,\n",
       "         ('major', 'expenses'): 1,\n",
       "         ('expenses', 'include'): 1,\n",
       "         ('include', 'Two'): 1,\n",
       "         ('Two', 'sets'): 1,\n",
       "         ('sets', 'of'): 1,\n",
       "         ('of', 'tyres'): 1,\n",
       "         ('tyres', ','): 1,\n",
       "         (',', 'Insurance'): 1,\n",
       "         ('Insurance', 'And'): 1,\n",
       "         ('And', 'fuel'): 1,\n",
       "         ('fuel', '.'): 1,\n",
       "         ('The', 'car'): 1,\n",
       "         ('car', 'has'): 1,\n",
       "         ('has', 'done'): 2,\n",
       "         ('done', 'over'): 1,\n",
       "         ('over', '110k'): 1,\n",
       "         ('110k', 'and'): 1,\n",
       "         ('and', 'still'): 1,\n",
       "         ('still', 'drives'): 1,\n",
       "         ('drives', 'like'): 1,\n",
       "         ('like', 'a'): 2,\n",
       "         ('a', 'dream'): 1,\n",
       "         ('dream', '.'): 1,\n",
       "         ('.', 'Toyota'): 1,\n",
       "         ('Toyota', 'on'): 1,\n",
       "         ('on', 'the'): 2,\n",
       "         ('the', 'other'): 2,\n",
       "         ('other', 'hand'): 2,\n",
       "         ('hand', 'being'): 1,\n",
       "         ('being', 'twice'): 1,\n",
       "         ('twice', 'as'): 1,\n",
       "         ('as', 'expensive'): 1,\n",
       "         ('expensive', 'feels'): 1,\n",
       "         ('feels', 'like'): 2,\n",
       "         ('a', 'premium'): 1,\n",
       "         ('premium', 'car'): 1,\n",
       "         ('car', 'and'): 1,\n",
       "         ('and', 'is'): 1,\n",
       "         ('is', 'rock'): 1,\n",
       "         ('rock', 'solid'): 1,\n",
       "         ('solid', 'trouble'): 1,\n",
       "         ('trouble', 'free'): 1,\n",
       "         ('free', ','): 1,\n",
       "         (',', 'worry'): 1,\n",
       "         ('worry', 'free'): 1,\n",
       "         ('free', 'car'): 1,\n",
       "         ('The', 'biggest'): 1,\n",
       "         ('biggest', 'expense'): 1,\n",
       "         ('expense', 'here'): 1,\n",
       "         ('here', 'was'): 1,\n",
       "         ('was', 'insurance'): 1,\n",
       "         ('insurance', '.'): 1,\n",
       "         ('.', 'This'): 1,\n",
       "         ('This', 'one'): 1,\n",
       "         ('one', 'has'): 1,\n",
       "         ('done', '25k'): 1,\n",
       "         ('25k', 'and'): 1,\n",
       "         ('and', 'feels'): 1,\n",
       "         ('like', 'We'): 1,\n",
       "         ('We', 'bought'): 1,\n",
       "         ('bought', 'it'): 1,\n",
       "         ('it', 'yesterday'): 1,\n",
       "         ('yesterday', '.'): 1,\n",
       "         ('.', 'In'): 1,\n",
       "         ('In', 'terms'): 1,\n",
       "         ('terms', 'of'): 1,\n",
       "         ('of', 'comfort'): 1,\n",
       "         ('comfort', 'Toyota'): 1,\n",
       "         ('Toyota', 'is'): 1,\n",
       "         ('is', 'far'): 1,\n",
       "         ('far', 'superior'): 1,\n",
       "         ('superior', 'than'): 1,\n",
       "         ('than', 'any'): 1,\n",
       "         ('any', 'car'): 2,\n",
       "         ('car', 'in'): 2,\n",
       "         ('in', 'the'): 2,\n",
       "         ('the', 'industry'): 1,\n",
       "         ('industry', 'be'): 1,\n",
       "         ('be', 'it'): 1,\n",
       "         ('it', 'the'): 1,\n",
       "         ('the', 'driver'): 1,\n",
       "         ('driver', 'or'): 1,\n",
       "         ('or', 'the'): 1,\n",
       "         ('the', 'passengers'): 2,\n",
       "         ('passengers', 'there'): 1,\n",
       "         ('there', 'is'): 1,\n",
       "         ('is', 'no'): 2,\n",
       "         ('no', 'match'): 1,\n",
       "         ('match', 'for'): 1,\n",
       "         ('for', 'Innova'): 1,\n",
       "         ('Innova', '.'): 1,\n",
       "         ('.', 'Honda'): 1,\n",
       "         ('Honda', 'city'): 1,\n",
       "         ('city', 'on'): 1,\n",
       "         ('hand', 'is'): 1,\n",
       "         ('is', 'superb'): 1,\n",
       "         ('superb', 'in'): 1,\n",
       "         ('in', 'driving'): 1,\n",
       "         ('driving', '.'): 1,\n",
       "         ('.', 'It'): 1,\n",
       "         ('It', '’'): 1,\n",
       "         ('’', 's'): 1,\n",
       "         ('s', 'steering'): 1,\n",
       "         ('steering', 'is'): 1,\n",
       "         ('is', 'so'): 1,\n",
       "         ('so', 'light'): 1,\n",
       "         ('light', 'that'): 1,\n",
       "         ('that', 'anyone'): 1,\n",
       "         ('anyone', 'can'): 1,\n",
       "         ('can', 'drive'): 1,\n",
       "         ('drive', 'it'): 1,\n",
       "         ('it', 'with'): 1,\n",
       "         ('with', 'ease'): 1,\n",
       "         ('ease', '.'): 1,\n",
       "         ('.', 'There'): 1,\n",
       "         ('There', 'is'): 1,\n",
       "         ('no', 'engine'): 1,\n",
       "         ('engine', 'sound'): 1,\n",
       "         ('sound', 'at'): 1,\n",
       "         ('at', 'idling'): 1,\n",
       "         ('idling', '.'): 1,\n",
       "         ('.', 'Super'): 1,\n",
       "         ('Super', 'smooth'): 1,\n",
       "         ('smooth', 'ride'): 1,\n",
       "         ('ride', 'and'): 1,\n",
       "         ('and', 'you'): 1,\n",
       "         ('you', 'reach'): 1,\n",
       "         ('reach', 'in'): 1,\n",
       "         ('in', 'comfort'): 1,\n",
       "         ('comfort', 'to'): 1,\n",
       "         ('to', 'your'): 1,\n",
       "         ('your', 'destination'): 1,\n",
       "         ('destination', '.'): 1,\n",
       "         ('.', 'Long'): 1,\n",
       "         ('Long', 'drives'): 1,\n",
       "         ('drives', 'for'): 1,\n",
       "         ('for', 'the'): 1,\n",
       "         ('passengers', 'is'): 1,\n",
       "         ('is', 'not'): 1,\n",
       "         ('not', 'as'): 1,\n",
       "         ('as', 'good'): 1,\n",
       "         ('good', 'as'): 1,\n",
       "         ('as', 'Innova'): 1,\n",
       "         ('Innova', 'though'): 1,\n",
       "         ('though', '.'): 1,\n",
       "         ('.', 'Overall'): 1,\n",
       "         ('Overall', 'to'): 1,\n",
       "         ('to', 'sum'): 1,\n",
       "         ('sum', 'up'): 1,\n",
       "         ('up', 'both'): 1,\n",
       "         ('both', 'are'): 1,\n",
       "         ('are', 'amazing'): 1,\n",
       "         ('amazing', 'brands'): 1,\n",
       "         ('brands', 'super'): 1,\n",
       "         ('super', 'reliable'): 1,\n",
       "         ('reliable', ','): 1,\n",
       "         (',', 'great'): 1,\n",
       "         ('great', 'resale'): 1,\n",
       "         ('resale', 'value'): 1,\n",
       "         ('value', 'for'): 1,\n",
       "         ('for', 'both'): 1,\n",
       "         ('.', 'Service'): 1,\n",
       "         ('Service', 'cost'): 1,\n",
       "         ('cost', 'is'): 1,\n",
       "         ('is', 'comparable'): 1,\n",
       "         ('comparable', 'to'): 1,\n",
       "         ('to', 'any'): 1,\n",
       "         ('the', 'segment'): 1,\n",
       "         ('segment', '.'): 1,\n",
       "         ('.', 'To'): 1,\n",
       "         ('To', 'decide'): 1,\n",
       "         ('decide', 'between'): 1,\n",
       "         ('between', 'one'): 1,\n",
       "         ('one', 'of'): 1,\n",
       "         ('of', 'the'): 1,\n",
       "         ('the', 'two'): 1,\n",
       "         ('two', 'is'): 1,\n",
       "         ('is', 'a'): 1,\n",
       "         ('a', 'difficult'): 1,\n",
       "         ('difficult', 'decision'): 1,\n",
       "         ('decision', 'as'): 1,\n",
       "         ('as', 'both'): 1,\n",
       "         ('both', 'the'): 1,\n",
       "         ('the', 'brands'): 1,\n",
       "         ('brands', 'are'): 1,\n",
       "         ('are', 'of'): 1,\n",
       "         ('of', 'amazing'): 1,\n",
       "         ('amazing', 'quality'): 1,\n",
       "         ('quality', '.'): 1})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams1_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score(gen_templates, target1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.025974025974025976, recall=0.2857142857142857, fmeasure=0.04761904761904762),\n",
       " 'rouge2': Score(precision=0.004347826086956522, recall=0.05, fmeasure=0.008)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.17316017316017315, recall=0.3669724770642202, fmeasure=0.2352941176470588),\n",
       " 'rouge2': Score(precision=0.02608695652173913, recall=0.05555555555555555, fmeasure=0.03550295857988165)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score(small, target1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.23809523809523808, recall=0.34375, fmeasure=0.28132992327365725),\n",
       " 'rouge2': Score(precision=0.02608695652173913, recall=0.03773584905660377, fmeasure=0.030848329048843187)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score(big, target1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.2857142857142857, recall=0.025974025974025976, fmeasure=0.04761904761904762),\n",
       " 'rouge2': Score(precision=0.05, recall=0.004347826086956522, fmeasure=0.008)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "scores = scorer.score(target1, gen_templates)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In terms of sheer corporate value, Toyota is the most successful, preponderant automaker in the world. Honda is much smaller with an overall value that's just 25 percent of Toyota's. Toyota also sells many more vehicles in the United States every year than Honda.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
